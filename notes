
Docker Mastery notes:

Course GitHub repo: https://github.com/BretFisher/udemy-docker-mastery

---

[n] - section

n > - chapter

---

[1] Quick start

1 > Principle: Build, Ship, and Run

Kubernetes vs Docker: https://www.bretfisher.com/kubernetes-vs-docker/
OCI overview: https://opencontainers.org/about/overview/
Read this lecture on GitHub:
	https://github.com/BretFisher/udemy-docker-mastery/blob/main/intro/what-is-docker/what-is-docker.md

Section commands and links: res/section-commands-and-links.pdf


2 > Quick container run
labs.play-with-docker.com
	Best way to practice online without installing anything!
	Login with DockerHub account

Docker Hub: https://hub.docker.com/
Docker official images: https://docs.docker.com/trusted-content/official-images/
Read this lecture on GitHub:
	https://github.com/BretFisher/udemy-docker-mastery/blob/main/intro/quick-container-run/quick-container-run.md

3 > Why Docker?, Why now in 2023?

A brief history of containers: https://www.aquasec.com/blog/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016/
Read this lecture on GitHub:
	https://github.com/BretFisher/udemy-docker-mastery/blob/main/intro/why-docker/why-docker.md


[2] Course instructions

5 > All examples and assignments:
	https://github.com/bretfisher/udemy-docker-mastery
	clone in user directory(/Users/<user> for Mac) - doesn't matter for linux
		~-> ensures docker works correctly on Mac/Windows

Docker Mastery slides: res/docker-mastery-slides-3.0.zip
Docker cheat-shit: res/docker-cheat-sheet-08.09.2016_0.pdf
Docker Mastery commands: res/docker-mastery-commands-2.0.zip


6 > Discord channel: "Cloud Native DevOps"
7 > YouTube channel: "Bret Fisher Docker and DevOps" - AMA every Tuesday
8 > The big FAQ - TODO


[3] The best way to setup docker for your OS

9 >
Docker Desktop - bundle of tools to run docker locally - best way to learn in the beginning
	subscription service, but free for learning even on company computer
OCI [Open Container Initiative] - standard for containers, images and registries
@8:55 - alternatives to Docker Desktop

OCI [Open Container Initiative]: https://opencontainers.org/

Docker Desktop alternatives:
	https://docs.google.com/spreadsheets/d/1ZT8m4gpvh6xhHYIi4Ui19uHcMpymwFXpTAvd3EcgSm4/edit?gid=0#gid=0
Download Docker Desktop: https://docs.docker.com/desktop/


11 > Docker for Windows setup and tips

Recommended: WSL2 over Hyper-V(old way)
Virtualization needs to be enabled in the BIOS.
Tweaks:
Docker Desktop > Settings > Resources
	> WSL Integration -> all WSL distros are shown here. You can enable Docker Desktop integration with any/all of them.

You can install WSL Ubuntu, create an account in it. Docker commands should work once Docker Desktop is running.
Windows Terminal is recommended.
Recommended to clone the course repo inside WSL.

Docker Desktop Windows installation: https://docs.docker.com/desktop/install/windows-install/


12 > Docker for Mac setup and tips

Like every other Mac app, you download it as a DMG, and just copy the "Docker.app" in the "Applications" folder.
Tweaks:
Docker Desktop > Settings >
	Resources > Advanced -> you can increase the limits here if Docker is running slow(shrunk when Docker is not running).
	Experimental Features -> feel free to enable these

Docker Desktop Mac installation: https://docs.docker.com/desktop/install/mac-install/


13 > Docker for Linux setup and tips

Docker Desktop still creates a tiny VM even on Linux for consistent experience across all the platforms.
Official Docker Dock steps are more involved that Windows/Mac
Tweaks:
Docker Desktop > Settings > Resources >
	Advanced -> you can increase the limits here if Docker is running slow(shrunk when Docker is not running).
	File sharing -> what directory(and all subdirectories) can be bind-mount inside the containers.


Docker Desktop Linux installation: https://docs.docker.com/desktop/install/linux-install/


14 > Docker for Linux server setup and tips

For running Docker remotely, if for some reason(performance/resources/licensing), you don't want to run it locally.
This is about *Docker Engine* for Linux servers.
Docker Engine + CLI != Docker Desktop.
	We had Docker Engine before we had Docker Desktop. Docker Desktop is a superset.
	Docker Enhanced is a single binary "dockerd" which runs on any Linux.

Just run the script with curl: get.docker.com
If you want to run it on a remote server, you don't have to ssh there, just install it there and
	install the docker-cli client locally(also included in Docker Desktop):
		- Mac, `$ brew install docker` just installs the client.
		- Linux: `$ apt install docker-ce-cli`
		- Windows: `$ choco install docker-cli`
	tell the client to connect to that server(instead of talking locally by default):
		- create environment variable: `$ export DOCKER_HOST=ssh://root@<IP-addr>` (it's blank by default => local)

		You can set different value in different terminals to talk to different servers.
		There's also `$ docker context --help` command which can also do this.

	Remember: whenever Bret types "localhost" in the browser to test something, you will have to replace it with the
		IP address of the remote server running the Docker Engine.


Docker Engine Linux server installation: https://docs.docker.com/engine/install/
Install Docker CLI locally: https://docs.docker.com/engine/install/binaries/#install-server-and-client-binaries-on-windows


15 > VS code 

Can also run on web: https://vscode.dev
Install, login, and turn on the sync!
Recommended extensions:
	Docker - Microsoft
	Kubernetes - Microsoft
	Remote Development - Microsoft, installs:
		Remote Container -> allows running your code inside a container anywhere the docker is running
		Remote SSH -> treats code on a remote server almost like you are developing on your local system
		Remote WSL -> allows vscode running natively on Windows to see files inside WSL
	Live Share - Microsoft -> allows you to live share your code through vscode and edit simultaneously



[4] Creating and using containers like a boss

17 >
`$ docker version` - to check if the client can talk to the server & is working properly
`$ docker info` -> config values of docker engine

New CLI style: docker <command> <sub-command> <options>	- better organized
Old CLI style: docker <command> <options> - still supported

res/s03-containers-slides.pdf
res/dm-s03-commands.txt

18 >
`$ docker containers run --publish 80:80 --detach nginx`	- `-p` format is <host:container>
	-> download & run nginx image from the default registry hub.docker.com
	-> always runs a *new* one & NOT an existing one.. use `start` for existing one

`$ docker container ls` -> running containers
`$ docker container ls -a` -> all containers
`$ docker container stop <container ID (initial unique part)>`

`$ docker container logs <container-name>`
`$ docker container top <container-name>` -> processes inside the container
`$ docker container rm <container-name>...` -> remove container(s)

20 >
People compare containers to VMs.. there are some similarities but there are so many things that aren't same
	let's not even compare them to VMs

A container ~ just a process running on host
	with limited resources
	exits when the process stops

`$ docker container top` -> processes "inside" the container
	this process can be seen running on the host(`$ ps -aux`) with different pid!

Mike Coleman(Docker employee) "Docker for the Virtualization admin" ebook:
	https://github.com/mikegcoleman/docker101/blob/master/Docker_eBook_Jan_2017.pdf
Docker internals: https://www.youtube.com/watch?v=sK5i-N34im8&list=PLBmVKD7o3L8v7Kl_XXh3KaJl9Qw2lyuFl
Docker for Mac - commands for getting into local Docker VM:
	https://www.bretfisher.com/docker-for-mac-commands-for-getting-into-local-docker-vm/
Docker for Windows - commands for getting into local Docker VM:
	https://www.bretfisher.com/getting-a-shell-in-the-docker-for-windows-vm/

21 >
Images are always OS & arch specific (of course)

22 > Assignment: managing multiple containers
23 > Assignment answers


24 > What's going on inside running containers:
`$ docker container top <container>` -> processes running inside container
`$ docker container inspect <container>` -> lot of data, how it was started etc.
`$ docker container stats <container>...` -> CPU/Mem etc. usage

25 > Use mariadb instead of mysql as mysql has removed the `ps` command

26 > Running a container and also getting a shell inside it, SSH not needed
`$ docker container run -it` -> run interactively
	-i => interactive
	-t => get tty to it

Ex. `$ docker container run -it --name proxy nginx bash` -> bash ~ optional root command to replace the default one
	`$ exit` -> the container exits when the root command exists

You can do that for distro containers!:
`$ docker container run -it --name ubuntu ubuntu` -> The command is by default bash for distros!
	They are minimal, so you can install whatever with apt-get and exit -> stops the container
	Alpine Linux is the smallest: <5MB, no bash only sh

>> Starting it again has all those packages installed!:
`$ docker container start -ai <container-name>` -> -ai(attach interactive) instead of -it

>> To attach to already running container:
`$ docker container exec -it <container> <cmd>`
	ex. `$ docker container exec -it ubuntu bash`
	`$ exit` won't stop the container as `bash` wasn't the root process

Linux package manager basics: https://www.digitalocean.com/community/tutorials/package-management-basics-apt-yum-dnf-pkg


27 > Docker Networks

Docker concept: "Batteries included, but removable"
	For local dev/testing, defaults of networks usually just work

Whenever you start a container, in the background, you are connecting to a particular docker network
	by default the bridge(or docker0) networks
	each of those networks rout out through a NAT firewall(on host IP), which is actually the docker daemon configuring
		the host IP address on it's default interface, so the containers can get out to the internet or rest of the network

But we don't need to use -p option when we have specific containers wanting to talk to each other inside our host
Best practice is to create a networks for each application, example:
	network "my_web_app" for mysql & php/apache containers
	another network "my_api" for mongo & nodejs containers

	Without exposing the ports with `-p` these containers can talk to each other within their networks
		but not outside of their virtual networks

	So for examples, the "my_web_app" network can have only the php/apache container exposing it's port and not the mysql one
		this way, they can talk to each other but only php/apache can talk to the outside world

But all of this is configurable
	you can create multiple virtual networks, may be 1 per app or based on security requirements
		like a real physical computer with two NICs you can have two virtual networks connected to one container
	Or you can have the container talk to no network
	Or you can skip the OOTB virtual network config and use `--net=host`
		you will lose some containerization benefits, but sometimes, it may be required


`$ docker container port <container>` -> shows port mapping like: 80/tcp -> 0.0.0.0:80 (part of `docker container ls` output)

We haven't talked about the IP address yet..

`$ docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost` -> container IP address
	- `-format '...'` is just for clean ouput(Go templates), can also use grep

@6:40 good hand drawn diagram explanation

Reminder: there can't be 2/more containers listening on the same port of the host

Docker `--format` option: https://docs.docker.com/config/formatting/


28 > Minor correction
nginx has now removed the ping command
	for the examples in the next videos, instead of nginx, use nginx:alpine image for the examples
	or install it with apt-get


29 > CLI management of virtual networks
Commands:
`$ docker network ls` -> network list
`$ docker network inspect` -> all details
`$ docker network create --driver` -> create new network optionally specifying a driver

`$ docker network connect`
`$ docker network disconnect`
	these commands change the live running container so that a new NIC is created on a virtual network for that container
	it's like sticking a NIC in a computer while it's running


Typical default networks:
	bridge/docker0 - the default bridge network
	host - the special network that skips the docker's virtual network & attaches the container directly to the host interface
		prevents the security boundaries of the containerization from protecting the container interface
		but in some cases, can improve the performance of high throughput networking etc.
	none - kind of equivalent of having an interface on the computer that's not attached to anything


Examples:
`$ docker network create my_app_net` -> creates new network with the default driver bridge
	the bridge driver simply creates a virtual network locally with it's own subnet somewhere around 172.17 & above
	goes incrementally like 18, 19, 20... it doesn't have any advanced features like overlaied networks that allow
	privet networking between hosts and other 3rd party drives like Weave

`$ docker container run --network my_app_net nginx` -> creates a new container in that network

`$ docker network connect <network> <container>` -> dynamically creates a NIC in the container on the given virtual network
	now the existing container is on two networks(assuming it was on one before)
`$ docker network disconnect <network> <container>`


"If you are running all the apps on a single server, you are able to really protect them. Because in the physical world we
would be creating VMs and hosts in a network, we would often overexpose the ports and networking on our application servers
But here, you would put all your app containers in one network in a virtual network, you will only expose the ports on your
hosts that you specifically use the `-p` with. And everything else is a little bit safer with that protected firewall inside
their virtual network"

So the front-end/back-end sit in the same Docker network and their internal communication never leaves the host.
All externally exposed ports are closed by default. You must manually export via `-p`, which is a good default security.

Later we will see Docker Swarm multi-host networking and how it gets better when we scale up and scale out.


30 > DNS

Forget IPs: Static IPs and using IPs for talking to containers is an anti-pattern. Do your best to avoid it.

As containers are constantly launching, disappearing, moving, expanding, and shrinking, we can't rely on IPs for talking.
No guarantee that the IP will remain the same any time.. a container my go down and Docker may bring it up somewhere else.
Too dynamic!
Builtin solution: DNS
	Docker uses the container names as the equivalent of host names for containers talking to each other.
	Docker defaults to container names as the host names but you can also set aliases.

The new network we created(not the default bridge one) gets a special new feature:
	auto DNS resolution for all the containers in that virtual network from all the other containers in that virtual network
		using their container names, regardless of their IP addresses

	`$ docker container exec -it cont-1 ping cont-2` - cont-1 can ping cont-2 in the same network

The default bridge network doesn't have the DNS server built into it by default.
	There, you can use the `~ container create --link <list>` option to manually specify links between the bridge network.
	But it's just easier to create a new network for your apps so you don't have to do this every time. (recommended always)
		And wit docker compose it gets even easier.

I think it works on the same hosts as well as across hosts. TODO: check

DNS basics blog & video: https://dyn.com/blog/dns-why-its-important-how-it-works/
DNS basics comic: https://howdns.works/


31 > Assignment: check versions of curl on Ubuntu 14.04 and CentOS 7
32 > Assignment answer
Just do:
	`$ docker container run -it --name ubuntu ubuntu:14.04`
	`$ apt-get update && apt install curl`
	`$ curl --version`
		-> 7.35.0

	`$ docker container run -it --name centos centos:7`
	`$ yum update curl`
	`$ curl --version`
		-> 7.29.0

33 > Correction in next assignment:
elasticsearch:2 is old and requires x86_64 architecture you can use "bretfisher/httpenv" image instead, a simple web server

34 > DNS round robin testing

DNS round robin => you can have two different hosts with DNS aliases that respond to the same DNS name
	like google.com having more than 1 server for being up 24/7 => multiple IP address beind the name used on the internet
	in containers, it gets a lot simpler

Docker engine 1.11 & above: we can have multiple containers on a created network respond to the same DNS name
Task:
	- create a new virtual network
	- in it, create two containers from elasticsearch:2 image
		research & use `--network-alias search`(of `docker container run`) option
			to give additional DNS name(along with it's container name) to respond to

			this solves a little problem: you can't add multiple containers with the same name, but how can you resolve
				DNS on the network and maybe have the same app installed twice.. maybe for dev & test env on the same
					Docker server and want to call both of them "search" in the DNS

	- use alpine image to use nslookup: `alpine nslookup search` with `--net` ~-> list of DNS addresses for the name search
	- run `centos curl -s search:9200` with `--net` multiple times until you see both "name" fields show
		elasticsearch gives itself random names when it first stats up(like containers)
		so when you run curl multiple times, the request can randomly go to any of them and you can identify by the name
		"it's not a true one but a poor man's load balancer"

I did:
`$ docker network create tmpnet`
`$ docker container run -d --network tmpnet --network-alias search --name else-1 elasticsearch:2`
`$ docker container run -d --network tmpnet --network-alias search --name else-2 elasticsearch:2`
`$ docker container run --rm --network tmpnet -it --name alpine alpine`
	`$ nslookup else-1` -> shows name "else-1" & it's IP address
	`$ nslookup else-2` -> shows name "else-2" & it's IP address
	`$ nslookup search` -> shows nane "search" & both the above IP addresses
	`$ apk --no-cache add curl`
	`$ curl -s search:9200`
	& repeat...
		-> request is randomly redirected to one of the servers

Then cleaned up the containers.

Round-robin DNS: https://en.wikipedia.org/wiki/Round-robin_DNS


35 > Answers:

These are interchangeable: --net & --network and --net-alias --network-alias

`$ docker network create dude`
`$ docker container run -d --net dude --net-alias search elasticsearch:2`
`$ docker container run -d --net dude --net-alias search elasticsearch:2`

`$ docker container ls`
	-> The ports shown are exposed inside the virtual network. `-p` is needed only for exposing the ports to the host.

`$ docker container run --rm --net dude alpine nslookup search`
	-> shows nane "search" & both the above IP addresses

`$ docker container run --rm --net dude centos curl -s search:9200`
& repeat...
	-> request is randomly redirected to one of the servers

Then cleaned up the containers.



[5] Container Images, Where To Find Them and How To Build Them

36 > What's an image (and what isn't)

What's an image: Application binaries and dependencies of your app and the metada about how to run it.
Official definition: ordered collection of root filesystem chages and the corresponding execution params
	for use withing a container runtime

There's no complete OS, no kernel, no kernel modules(like drivers)
It can be as small as 1 file(your app binary) like a Go static binary or
	as big as an entire distro with it's package manager, apache, PHP, source code, whatever installed packages.. multi GBs!


Official Docker image spec: https://github.com/moby/moby/blob/master/image/spec/v1.md
	moved to: https://github.com/moby/docker-image-spec

Commands: res/dm-s04-commands.txt



37 > The Docker Hub

Sign in -> dashboard -> shows your repos(public/private), you won't have any yet.
Search: nginx -> huge results
	on the top there's the official image (It has the word "official" below it & It's name is plain & doesn't contain '/')
	all other images names look like â‰ˆ <account-name>/nginx
	Docker Inc. has a team of people for the official ones that usually works with the official team that makes that software
		~-> proper documentation, well tested, their Dockerfiles follow best practices
Official images are the best way to start.

Their versions(nginx example):
- 1.11.9, mainline, 1, 1.11, latest (mainline/jessie/Dockerfile)
- 1.11.9-alpine, mainline-alpine, 1-alpine, 1.11-alpine, alpine (mainline/alpine/Dockerfile)
- 1.10.3, stable, 1.10 (stable/jessie/Dockerfile)
- 1.10.3-alpine, stable-alpine, 1.10-alpine (stable/alpine/Dockerfile)

Each bullet above is one version with all those tags referring to that same version
	Unless you specify the exact version, you automatically get the latest of the unspecified part
	It's best to use the exact versions(like nginx:1.11.9)
		as you will usually wanna control the upgrade process with some other devops tool

jessie is a Debian distro, alpine is an extremely small Linux distro: < 5MB

"latest" is a special tag. Plain image name(like nginx) means the latest version

`$ docker image ls` -> shows all the downloaded images
	entries with the same image id(cryptographic SHA) are actually only one image

Docker hub is like GitHub. Anyone can create new/modify existing images and upload their own images
	number of stars & pulls is a good indicator of their quality.. you can check the source code if the link is provided.


List of official docker images: https://github.com/docker-library/official-images/tree/master/library


38 > Images and their layers: discover the image cache

Union file system:
	Images are made up of layers. Evert image starts with a blank layer called scratch
	and then every set of change that happen after that on the FS in the image is another layer.
	There can be dosens of them, some may be empty with no change(only metadata change).
	Every layer has it's own unique SHA

`$ docker image history <image-with-tag>` -> history of layers


Example: 
	You may have Ubuntu layer at the very bottom.
	Then you create a Dockerfile which add some more files -> another layer
	Then you make an env variable change -> another layer

	If there's another image that also uses Ubuntu, with it's own changes on top of it,
		the same Ubuntu layer in your cache is shared.

	=> no duplicate storage, download, or upload


When you run an image, the docker creates a new read/write layer for the running container on top of the image layers.
It follows COW. When you make change to a file in a running container, the file is copied in the container layer
	and then modified there.


`$ docker image inspect <image>` -> all image details(the metadata part of the binaries & metadata that the image contains)
	exposed ports, env variables, cmd, author, arch and much more


Docker docs on images & containers: https://docs.docker.com/storage/storagedriver/


39 > Image tagging and pushing to Docker hub

`$ docker image tag --help`

Images don't technically have names. `$ docker image ls` output has not name column. It has repository, tag, & image ID
Besides the image ID(which we don't remember), we have to refer to them with three different pieces of information.

Repository: <username/org>/<repository> (only <repository> for official images as they live at registry's root namespace)
	example:
		bretfisher/nodemongoapp
		mysql						<- official

Tag: Not quiet a version or branch. It's kind of like git tags.. kind of little bit of both.
	It's really just a pointer to a specific image commit and could be anything into that repository

How do we create new layer?
	You can create your own Dockerfile and create your own custom image
	But we can also retag existing images

`$ docker image tag <source-image>[:<tag>] <target-image>[:<tag>]`

Default tag is "latest" which doesn't necessarily mean latest software. You can take old software and tag it "latest".
	It could just have been called "default".
	But you can trust the official images to contain the latest stable version of the software.


Example: `$ docker image tag nginx bretfisher/nginx` -> creates new tag with same image ID

But it doesn't yet exist on the Docker Hub. You need to push it.
`docker push` uploads changed layers to an image registry(default is Hub)

Needs login:
`$ docker login` -> stores authentication key to ~/.docker/config.json(now Mac stores in Keychain for better security)
`$ docker logout` -> do this to remove it if you don't trust the machine.. always do this on shared machines/servers

`$ docker image push bretfisher/nginx` -> uploads the layers, Docker Hub dashboard now shows your new image(tag)

Now, if you assign yet another new tag & push:
`$ docker image tag bretfisher/nginx bretfisher/nginx:testing`
`$ docker image push bretfisher/nginx:testing` -> says "layer already exists" for all the layers => no duplicate upload
	Docker Hub dashboard now shows both the tags

Similar to GitHub, you can create a private repository and push to it.


40 > Building images: Dockerfile basics

Dockerfile(default name): recipe for creating images. Made up of stanzas. Each stanza is a layer => top-down order matters.

course-git-repo:dockerfile-sample-1/Dockerfile


`FROM` is must. Distro images are mainly used for their package managers. Alpine is common these days.
`FROM scratch` => start with an empty container

`ENV <key> <value>`

`RUN <bash-command>` -> mainly for installing packages, unzipping, editing file inside the container
	can also run scripts copied earlier in file (any command accessible inside the container at that point in the file)

As each stanza has it's own layer, `RUN <cmd> && <cmd> && <cmd>` -> all commands in the same layer.

By default, no TCP/UDP ports are open inside a container. It doesn't expose anything from a container to the virtual network
unless listed:
	`EXPOSE <port>s...`

nginx example: `EXPOSE 80 443` - as nginx is a web & proxy server

	This doesn't mean these ports are gonna be opened automatically on our host. That is done by the `-p` option

`CMD ["<cmd>", "<option>"s...]` - required, final command to run every time a new container is launched (or stopped restarted)


Dockerfile command details: https://docs.docker.com/reference/dockerfile/


41 > Building images: running docker builds

`$ docker image build -t customnginx .` - using locally without pushing to Hub.. so no need of account name before the name
	runs each stanza and prints a hash for each => each layer stored in the cache
		=> next time if the line isn't changed, it won't be rerun
	
	If a stanza changes, every stanza from that onwards is rerun, all previous ones are just "using cache" when building

	That's why the order matters.
		Best practice: things that change the list go at the top, things that change the most go at the bottom of the file.


42 > Building images: extending official images

course-git-repo:dockerfile-sample-2/
	there is a Dockerfile and an index.html

Dockerfile:
	FROM nginx:latest					# using official images -> lot easier to maintain this Dockerfile
	WORKDIR /usr/share/nginx/html		# just cd into that directory. Recommended over `RUN cd ...`
											# here we are changing nginx's default dir to html files
	COPY index.html index.html			# copy our custom homepage into the image
	# No need of `CMD` as it's in the `FROM`ed image. We inherit everything from there

`$ docker image build -t nginx-with-html .` - very quick as nginx was already in the cache

You can check the original nginx home page changed in this one when you run a container from it.

You need to tag it to push to the Hub:

`$ docker image tag nginx-with-html:latest bretfisher/nginx-with-html:latest`

43 > Assignment
course-git-repo:dockerfile-assignment-1/Dockerfile

44 > Assignment answer

45 > Using prune to keep the docker clean

His YouTube video: https://youtu.be/_4QzP7uwtvI

`$ docker image prune` -> cleans up just the 'dangling' images
`$ docker system prune` -> cleans up everything you are not using currently
`$ docker system df` -> space usage



[6] Persistent data: volumes

46 > Container lifetime & persistent data

Containers are meant to be immutable & ephemeral => temporary
"immutable infrastructure" => only re-deploy containers, never change
Ideally, the container shouldn't contain the data generated by the app within - "separation of concerns"

The containers we are using so far are persistent: any change in them were kept across restarts,
	it's only when we removed the container, the UFS layer went away

But we want to be able to do that at will.
This problem of "persistent data" is new to containers as traditional apps have been persistent by default.

Two solutions:
1. Data volumes: create special location outside the container UFS to store data, preserved across container removals and
	can be attached to any container. Container sees it like a local file path.
2. Bind mounts: sharing of host directory or file into the container.
	Container sees it like a local path and doesn't know that it is coming from the host.

Intro to immutable infra concepts: https://www.oreilly.com/radar/an-introduction-to-immutable-infrastructure/
The 12 factor app: https://12factor.net/
12 fractured app: https://medium.com/@kelseyhightower/12-fractured-apps-1080c73d481c#.cjvkgw4b3
Docker storage info in Docker docs: https://docs.docker.com/storage/
res/dm-s05-commands.txt
res/s05-volumes-slides.pdf


47 > Data volumes

Example: official mysql Dockerfile:
`VOLUME /var/lib/mysql` - creates a new volume location & assign to this dir(the default mysql DBs location) in the container
	all the files there will outlive the container until we manually delete the volume

`$ docker inspect <container>` -> info displayed in: "Config" > "volume" and "Mounts" > {}
	- also shows location on host and other details
`$ docker volume ls` -> lists volume IDs
`$ docker volume inspect <volume-id>`

The CLI equivalent of the Dockerfile `VOLUME /var/lib/mysql` stanza:
`$ docker container run ... -v /var/lib/mysql`

But you don't get clear idea of what volume is what based on just IDs. Rather, you can create a named volume:
`$ docker container run ... -v msql-db:/var/lib/mysql`
	-> friendlier name(and also the source location)

You can also create volumes separately:

`$ docker volume create` - allows to specify a different driver(the default is "local")


48 > Shell differences for path expansion

When using shells parameter expansion instead of a literal path(like `$(pwd)`) remember the shell differences
Linux/macOS bash, sh, zsh, and Windows Docker Toolbox Quickstart Terminal: `$(pwd)`
PowerShell:		`${pwd}`
cmd.exe:		`%cd%`


49 > Bind mounts

Map host file or dirs into a container file or directory. Skips the UFS and lives beyond the containers.
	If the mapped file already exists in the container, the host file is seen while the bind mount exists.
		Running the container without the bind mount will show the file in the container.

Can't use in Dockerfile, must be in `$ docker container run` as bind mounts are host specific and need specific data
	to be on the host HDD in order to work.

`$ docker run ... -v /Users/sam/stuff:/path/in/container` (Linux/Mac)
`$ docker run ... -v //c/Users/sam/stuff:/path/in/container` (Windows)

	Note: It's considered as bind mount instead of a named volume if it includes a '/'

Example: from the previous assignment: course-git-repo:dockerfile-sample-2/

`$ docker container run -d --name nginx -p 80:80 -v $(pwd)/:/usr/share/nginx/html nginx`

Now changing the files on the hosts reflects the changes into the container and vice versa!
Great for development environments.


50 > DB passwords in containers (update for further course videos)

DBs need passwords but the mysql image & few others like redis had always allowed to
	just do `$ docker run` & run without a password

But later it changed.
Now you need to either set a password: `POSTGRES_PASSWORD=mypasswd` or
tell it to ignore password: `POSTGRES_HOST_AUTH_METHOD=trust`

This wouldn't break if the versions were pinned.
	But Bret only pins to the minor version in the course: 9.6 rather than 9.6.16
	for to prevent major changes from breaking yet also using the latest patches.

In this *very rare case* the official postgres maintainer decided to introduce a breaking change
	in the *image* to a patch release of the app. The two aren't related.

"This is a weakness of the Docker Hub model. There's no version of the Docker Hub image really, it's just tracking
the upstream postgres versions.. so then if any Docker Hub change would break something, it can't easily be tracked as
a separate version from the app itself."

So, always pin the whole image version for the things you care about.


51 > Updates for the next videos

Where Postgres "old" & "new" versions are mentioned, you can use postgres:15.1 and postgres:15.2.

52 > Assignment: volumes


Container best practice:
	Don't update the applications in the container, but rather replace the container with the new version.

Launch postgres 9.6.1 container with a named volume, refer Docker Hub for the path.
Container displays many initial logs.
Stop the container.
Launch a new container with postgres 9.6.2 with the same volume.
Very few logs are displayed, as the volume data persists.

53 > arm64 and Apple Silicon need newer versions of Postgress which need a password: `-e POSTGRES_PASSWORD=mypassword`

54 > Assignment answer

`$ docker container run -d --name psql1 -v psql:/var/lib/postgresql/data postgres:9.6.1`
`$ docker container logs -f psql1` -> more logs
`$ docker container stop psql1`
`$ docker container run -d --name psql2 -v psql:/var/lib/postgresql/data postgres:9.6.1`
`$ docker container logs -f psql2` -> less logs


55 > File permissions across multiple containers

Docker Desktop automatically translates permissions from your host(MacOS/Windows) into container(Linux)

But when working on pure Linux servers with just `dockerd`, no translation is made.
(Pure Linux hosts like production server setups).
File permissions problems can occur with container apps not having the permissions they need:
	- multiple containers accessing the same volume.
	- bind mounting existing files into containers.

File ownership between containers and host are just numbers.
Some commands(`ls -l`) show friendly usernames which are just name-to-number aliases from `/etc/passwd` `/etc/group`.
The kernel only deals with the IDs. These files(i.e. the mappings) will be different in different containers and the host.
No issues when a container is accessing it's own files. But for multiple containers accessing the same volume or bind-mount,
problems can arise in 2 ways:
1. `/etc/passwd` is different on across containers.
Different friendly usernames will be shown for the same file because of the different mappings.
So, Bret only cares about the IDs when trying to sunc up permissions.
"Different names are fine, because it's only ID that counts.
	Two processes trying to access the same file, must have a matching user ID or user group ID."

2. Your two containers are running as different users.

Maybe the user/group IDs and/or the USER statement in your Dockerfiles are different,
	and the two containers are technically running under different IDs.
Different apps will end up running as different IDs.
	For example, the node base image creates a user called node with ID 1000,
		but the NGINX image creates an nginx user as ID 101.
Also, some apps spin-off sub-processes as different users.
	NGINX starts its main process (PID 1) as root (ID 0) but spawns
		sub-processes as the nginx user (ID 101), which keeps it more secure.

Troubleshooting methods of Bret:
1. `$ ps aux` in each container -> list of processes and usernames.
	The process needs a matching user ID or group ID to access the files in question.
2. Find the UID/GID in each containers `/etc/passwd` and `/etc/group` to translate names to numbers.
	You'll likely find there a miss-match, where one containers process originally wrote the files with its UID/GID
	and the other containers process is running as a different UID/GID.
3. Figure out a way to ensure both containers are running with either a matching user ID or group ID.
	This is often easier to manage in your own custom app (when using a language base image like python or node)
	rather than trying to change a 3rd party app's container (like nginx or postgres), but it all depends.
	This may mean creating a new user in one Dockerfile and setting the startup user with USER
	(USER docs: https://docs.docker.com/reference/dockerfile/#user).
	The node default image has a good example of the commands for creating a user and group with hard-coded IDs
	(https://github.com/nodejs/docker-node/blob/6256caf2c507e7aafdeb8e7f837bab51f46f99e0/17/bullseye/Dockerfile#L3-L4):
	```
	RUN groupadd --gid 1000 node \\
			&& useradd --uid 1000 --gid node --shell /bin/bash --create-home node
	USER 1000:1000
	```

Note: When setting a Dockerfile's USER, use numbers, which work better in Kubernetes than using names.
Note 2: If `ps` doesn't work in your container, you may need to install it.


56 > Assignment: bind mounts

Use the Jekyll static site generator to start a local web server.
course-git-repo:bindmount-sample-1 - source code on host.
Jekyll container watching those file and auto detecting the updates.

`$ docker container run -p 80:4000 -v $(pwd):sitebretfisher/jekyll-serve` from that directory

Make changes to the file under `_posts` dir.
See the changes reflected on the web page.
And look at the logs in the container.

Jkyll site: https://jekyllrb.com/

57 > Answer (follow along)



[6] Making It Easier with Docker Compose: The Multi-Container Tool


58 > Docker Compose and the docker-compose.yml

Configure relationships between containers
Save container run settings in file
Two parts:
	docker-compose.yml file(default name or `-f`): containers, networks, volumes
	`$ docker-compose` - used normally for local dev/test

The format has it's own versions(first line of the file): 1 - Initial "Fig" version, 2, 2.1, 3, 3.1
	Starting with v1.3 these fiels can even directly be used with the `docker` CLI in production with Swarm.

No version => 1.0 -- less features -- not recommended -- Bret starts with minimum 2 and 3/3.1 if needed

Main sections:
services: they are really just containers -- called services as there can be multiple of those containers for redundancy.
```yml
services:
  myservicename: # can be whatever friendly name. This is also DNS name inside network
    image: # optional if you use build
	command: # optional -- replace the default image cmd
	environment: # optional --same as `-e` option
	volumes: # optional -- same as `-v` option

  myservicename2:

volumes: # optional -- same as `$ docker volume create`

networks: # optional -- same as `-v` in `$ docker run`
```

Basically this is for not having to type all the commands over and over again, but better than shell scripts.
Example from Jekyll section(same as `$ docker container run -p 80:4000 -v $(pwd):/site bretfisher/jekkyll-serve`):
```
version '2'

services:
  jekyll:
    image: bretfisher/jekkyll-serve
	volume:
	  - .:/site
	ports:
	  - '80:4000'
```

A sample Wordpress setup:
```
version: '2'

services:

  wordpress:
    image: wordpress
	ports:
	  - 8080:80
	environment:
	  WORDPRESS_DB_PASSWORD: example
	volumes:
	  - ./wordpress-data:/var/www/html

  mysql:
    image: mariadb
	environment:
	  MYSQL_ROOT_PASSWORD: example
	  more_keys: can-be-added-like-this
	volumes:
	  - ./mysql-data:/var/lib/mysql		# mysql in mariadb ?!
```


The YAML format quick reference: https://yaml.org/refcard.html
The compose file version differences: https://docs.docker.com/compose/compose-file/legacy-versions/
Docker compose release downloads(for Lunux users that need to download manually): https://github.com/docker/compose/releases
res/s06-compose-slides.pdf
res/dm-s06-commands.txt
YAML official website: https://yaml.org/
YAML vs JSON: https://nodeca.github.io/js-yaml/


59 > Compose V2 (released 2022)

Just remove the '-': `$ docker-compose` -> `$ docker compose`

Fully backward compatible, drop-in replacement for V1.
V1 was in Python, now rewritten in Go.
Faster, more stable.


60 > Basic compose commands

`docker-compose` CLI tool is a separate binary from the `docker` binary
	it comes bundled on Windows/Mac, on Linux, you can download it separately

It's not designed to be a production grade tool. It's ideal for local development

Example: course-git-repo:compose-sample-2 directory
`$ docker compose up` -> starts everything with logs in foreground
	each container's log in different color!!

Ctrl-C to stop it.

`$ docker compose up` -> starts everything, detached in the background, no logs
`$ docker compose logs` -> same logs
`$ docker compose down` -> stop & clean up

`docker-compose` talks to Docker API in the background just like the `docker` command.

Many options are similar:
`docker compose ps`
`docker compose top`


Compose vs. Swarm for production: https://github.com/BretFisher/ama/discussions/146
	(The selected answer of Bret recommends Swarm)


61 > Version dependencies in milti-tier apps

course-git-repo:compose-assignment-1 directory

When composing multiple containers like an app and a db, their version dependencies need to be checked.
	i.e. which version of the app needs which version of the db.

Use following versions in next assignments:
	drupal:9
	postgres:14

62 > Correction

There are two compose assignments, first one is in the `compose-assignment-1` directory.
In the answer video, Bret is mistakenly using the `compose-assignment-2` directory.


63 > Compose assignment 1

Build a basic Drupal content management site with postgres (2 services).
Expose Drupal on 8080
Recommended version: 2
For postgres, set a password: POSTGRES_PASSWORD
Work through the Drupal setup(asks for DB server default localhost won't work)
	remember: service name is DNS name
Extra credit: Drupal docker doc: it uses volumes to store the uploaded data. Experiment with that.


Compose file reference: https://docs.docker.com/compose/compose-file/
Don't use links, it's a legacy feature: https://docs.docker.com/network/links/


64 > Compose assignment 1 Answer


65 > Adding image building to compose files

`$ docker compose up` can build images at runtime if mentioned so and if not found in cache => won't build every time.

`$ docker compose build` or `$ docker compose up --build` to rebuild if you change them


Example course-git-repo:course-sample-3 directory:
```
version: 2

services:
  proxy:
    build:								# build custom image
	  context: .						# in this dir
	  dockerfile: nginx.Dockerfile		# from this docker file
	image: nginx-custom					# and name the image this
	ports:
	  - '80:80'
  web:
    image: httpd
	volumes:
	  - ./html:/usr/local/apache2/htdocs
```

`docker compose down --help` -> for options to auto delete the built images.

Compose names the containers, volumes, and networks with the name of the directory as the project name(can be configured).
It also does that for the built images. So `image` is optional. This makes it easier to auto remove these images with `down`.

`docker compose down --local` -> deletes those auto-named images
`docker compose down --all` -> deletes all the images i.e. including the httpd one


Compose build doc: https://docs.docker.com/compose/compose-file/build/


66 > Correction: using MariaDB rather than PostgreSQL

Sometimes mysql has support issues for arm64 image version(Apple Silicon and Raspberry Pi) and
	postgres isn't easily supported in newer Drupal versions.


MariaDB has better support and recommended for the next assignment. Only following env variables need to be changed:
```
MARIADB_ROOT_PASSWORD
MARIADB_DATABASE
MARIADB_USER
MARIADB_PASSWORD
```

67 > Compose assignment 2
course-git-repo:compose-assignment-2 directory

Continuing assignment 63, instead of using the official one directly, build custom drupal image with a custom template

68 > Compose assignment 2 answers

Some cool tips here:
	Clean up the cache after apt-get install -> reduces images size
	Example:
		RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

	If cloning a git repo in an image, just clone the single branch required
	Example:
		RUN git clone --branch <branch> --single-branch --depth 1 <repo> \
		&& chown -R www-data:www-data bootstrap
			permission change needed as docker runs are root but apache expects "www" permissions




[8] Swarm Intro and Creating a 3-Node Swarm Cluster


69 > Swarm mode: built-in orchestration

Not related to Swarm "classic" for pre-1.12 versions.
Added in 1.12 (summer 2016) via SwarmKit toolkit.
Enhanced in 1.13 (January 2017) via Stacks and Secrets.

Containers promise to be able to easily deploy the apps on anyone's hardware: virtual, physical, cloud, wherever
	and it works the same.

It's like we are a platform service, like Heroku or something.
But without all those platform features, how to we deploy/maintain 100s or 1000s of containers on many servers/nodes?

=> New problems that weren't previously problems for small organizations.

- How do you automate container life cycle?
	Netflix and all have many people, but how does a team of a couple of people or a solo devs
	scale them or deal with their entire life cycle? Like deploying/starting/restarting/recreating/deleting/updating etc.
- How can we scale out/in/up/down?
- How can we ensure our containers are recreated when they fail?
- How can we update containers without downtime(blue/green deploy)?
- How can we control/track where containers get started?
- How can we create cross-node virtual networks?
- How can we ensure the containers are only running on the machines that we intend for them to run on?
- How can we store secrets, keys, passwords and get them to the right container(and only that container)?
- How can we ?


Swarm mode is a server clustering solution brings together different OSes or hosts/nodes into a single manageable unit
	not enabled by default to not break the older setups.

Some concepts

Manager nodes with local copies of Raft DB to store config and the info needed by them to have the authority in a swarm.
	Encrypt their traffic(each one has TLS and own cert)

Worker nodes(each with TLS)

Each one of theses would be a VM or PM running some distro of Linux or Windows Server.

Something called "Control Plane" ~-> how orders get sent around the swarm, partaking actions.

Manager nodes issue orders down to the workers. A manager is also a worker with permissions to control the swarm.


Our normal `$ docker run` command can just deploy 1 container on the machine the Docker CLI is talking to
	local or maybe a server we have logged in to. It can't scale out/up. We need new commands to deal with that.

-> `$ docker service` command replaces `$ docker run` in Swarm
	-> extra features like how many replicas of the containers to run
	known as tasks

1 service - multiple tasks. Each task will launch a container.

Managers decide where in the swarm to place the replicas.
	By default, it tries to spread them out. Each node will get a container instance


Docker 1.12 Swarm Mounts deep dive
	part 1: https://www.youtube.com/watch?v=dooPhkXT9yI
	part 2: https://www.youtube.com/watch?v=_F6PSP-qhdA
Heart of SwarmKit - topology management: https://speakerdeck.com/aluzzardi/heart-of-the-swarmkit-topology-management
Heart of SwarmKit - store, topology, and object model: https://www.youtube.com/watch?v=EmePhjGnCXY&themeRefresh=1
Raft consensus visualization https://thesecretlivesofdata.com/raft/
res/dm-s07-commands.txt
res/s07-swarm-intro-slides.pdf


70 > Create your first service and scale it

Wherever we are running the docker, we can always create a one-node swarm, for our own testing purpose

`$ docker info` -> "Swarm: inactive" => Swarm is not enabled

`$ docker swarm init` -> now we have a single node swarm

It does:
	- a lot of PKI and security automation stuff:
		- creates a root certificate for the swarm to establish trust & sign certificates for all nodes & managers
		- special certificate is issued for the first manager node
		- create join-tokens to use on other nodes to join the swarm
	- enables the swarm API
	- creates the Raft consensus DB on disk to stores swarm config, root CA, & secrets
		- encrypted by default(1.13+)
		- no need for another key/value system to hold orchestration/secrets
		- replicates logs among the managers via multiple TLS in "control plane"

	- waits for any other nodes before it starts replicating the DB over to them

Raft is a protocol that ensures consistency across multiple nodes and is ideal for using in cloud
	where we can't guarantee that any one thing will be available for any moment in time.

`$ docker node ls` -> shows just the manager node that we created marked as leader(can be only one among all managers)

Just a quick look at: `$ docker node --help` and `$ docker swarm --help`

For now, look at: `$ docker service --help`

Unlike the single-host `$ docker run`, `$ docker service` is for cluster. We don't much care about individual nodes.
	We don't probably name them. We treat them like cattle("pet's vs cattle" analogy). They are just number.
	We just throw requirements at the swarm in the form of services, and it will orchestrate & decide which node etc.

Simple example:
`$ docker service create alpine ping 8.8.8.8` -> service ID
`$ docker service ls` -> shows the table with our service with random name(just like individual containers),
	1(actually running) of 1(you specified) replica is spun up - goal of the orchestrator is to make these numbers match,
		whatever that takes.

`$ docker service ps <service-name/id>` -> tasks(or containers) of the service
`$ docker container ls` -> still works

Let's scale it up!
`$ docker service update <service> --replicas 3`
Now `service ls` shows 3/3 and `service ps` shows 3 tasks(containers)

There's also `$ docker update` command for normal `docker run` containers
	to update certain container variables without having to kill and restart them
	most of them are related to controlling container resource usage

But `$ docker service update --help` has lot more options cause the goal is to replace containers and update changes
	without taking the entire thing down.

	You could technically take down one at at time to change and do rolling update(blue/green pattern).
	But the swarm will ensure that the way we update them is in a pattern that ensures consistent availability.


Now, what if you kill a container behind the back of the swarm?
`$ docker container rm -f <a-swarm-node>`

The swarm is gonna identify that and gonna launch a new one within seconds !!
	`$ docker service ls` will reflect 2/3 then 3/3
	`$ docker service ps <service>` shows the history

Cool!

`$ docker service rm <service>` -> takes everything down

Deploy services to a swarm (Docker Docs): https://docs.docker.com/engine/swarm/services/


71 > CLI change in 2017

`$ docker service create/update` now runs in foreground by default.. `--detach` needs to be used explicitly for background


72 > Use "Multipass" to create Docker, Swarm, and K8s VMs

"docker-machine" was used to create VMs with docker pre-installed.
The project is now archived(https://github.com/docker/machine/releases/tag/v0.16.2).
	But you can still use it for the course.
Today, there are better alternatives like https://multipass.run
Bret's discussion about multipass.run on his live show: https://www.youtube.com/watch?v=0Jipb9fhpIw&t=641s

Use multipass to create VMs for Swarm or K8s clusters.
Multipass creates full Ubuntu server VM on your Host machine
	using various virtualization options (hyper-v, VirtualBox(default), hyperkit, etc.).
	Fast and easy. Their website has a quick walkthrough for each host OS type.
Then you can install Docker and/or K8s inside them.
	`$ multipass shell <name>` -> VM's shell
	`$ multipass mount` -> to connect a host directory in to the VM
	`$ multipass transfer` -> to copy files in


73 > Create a 3-node swarm cluster
Options:
	1. play-with-docker.com
	2. docker-machine + Virtual Box comes with docker for Windows/Mac(can be downloaded for Linux)
		Just install VirtualBox and
		`$ docker-machine create node1`
		`$ docker-machine create node2`
		`$ docker-machine create node3`

		then ssh using `$ docker-machine ssh node1`
		or you can: `$ docker-machine env node1`
		`$ eval(docker-machine env node1)` will reprogram your host's docker CLI
			to talk to node1's docker host instead of the local one. Check `$ docker info` "Name" to see the host's name

	3. Digital ocean + Docker install (with Bret's referral code for free $10)
		You can create "droplets"(VMs) - default Ubuntu recommended
		$10/month is recommended ($5/month will work to be might get slow for later demos)
		Choose location near to you

	4 Roll your own 3 machines and use get.docker.com to install docker on 'em.


Requirement:
	All 3 machines need solid networking to each other and specific ports open.

Steps:
node1:
- `$ docker swarm init --advertise-addr <machine's public IP address>` - node1 becomes the leader manager.
- copy the swarm join command(for worker not manager) from the output and run that on node2.
- node1: `$ docker node ls` -> Manager status: node1: "Leader", node2: blank
- node2 can't use any of the swarm commands as workers aren't really privileged, can't control the swarm.
- `$ docker node update --role manager node2` -> promotes node2 to manager
- node1: `$ docker node ls` -> now it's manager status is "reachable"
- `$ docker swarm join-token manager` -> join-tokens are part of the swarm config, can also change them if compromised.
- run that on node3 for it to join as a manager.
- `$ docker node ls`

Now we have a 3-node redundant swarm with 3 managers. Now try this:

- `$ docker service create --replicas 3 alpine ping 8.8.8.8`
- `$ docker service ls`
- `$ docker node ps`
- `$ docker service ps <service>`




Docker Swarm Firewall ports: https://www.bretfisher.com/docker-swarm-firewall-ports/
SSH config: https://www.digitalocean.com/community/tutorials/how-to-configure-custom-connection-options-for-your-ssh-client
Windows Hyper-V driver for docker-machine: https://docs.docker.com/desktop/
Create and update SSH keys to Digital Ocean: https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/
Bret's Digital Ocean referral for $200 credit: https://www.digitalocean.com/?refcode=ee97875d52fa&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=CopyPaste



[9] Swarm basic features and using them in workflow

74 > Section requirement and version corrections

Bret will use the multi node cluster created in the last section. But if that's too much to build/maintain,
	we can always use a single node cluster. Nearly everything works the same,
	except that the scheduler will assign service replicas to different nodes if you have multiple.

Use below versions in docker commands & YAML due to breaking changes in Drupal:
drupal:9, postgres:14


75 > Scaling out in overlay networks

Just `--driver overlay` when creating the network -> Swarm wide bridge network
	containers across hosts on same bridge network can access each other kind of like they are on aa VLAN.
This is only for intra-swarm comm.
Can enable full network encryption using IPSec(AES), off by default for performance.
Each service can be added to 0/more overlay networks depending on your design.
	Traditional example: DB on back-end n/w, web server on front-end n/w and APIs on both n/w.

The only kind of network we can use in a swarm, because it allows us to span across nodes
	as if they are all on the local network.

Example:
`$ docker network create --driver overlay mydrupal`
`$ docker network ls`
`$ docker service create --name psql --network mydrupal -e POSTGRES_PASSWORD=mypass postgres`
`$ docker container logs <node id>`

Our DB is now running on node1

`$ docker service create --name drupal --network mydrupal -p 80:80 drupal`
`$ watch docker service ls` -> Linux's watch command
`$ docker service ps drupal`

Our website is now running on node2

How do they know how to talk to each other? -> using the service names
	In drupal's DB setup, you can use the DB service name `psql` as the DB's hostname


Cool thing: the website opens on all 3 IP address(of all 3 nodes) in the browser!
But it's running on only one node and
`$ docker service inspect` -> it has only 1 IP address

res/s08-swarm-basic-features-slides.pdf
res/dm-s08-commands.txt


76 > Scaling out with routing mesh

Routing mesh is incoming(or ingress network) that distributes packets for a service to it's tasks
	- Spans all the nodes
	- Uses IPVS from Linux kernel
	- Load balances services across their tasks
	- Two ways it works:
		1. Container-to-container
			If back-end DB service were increased to 2/more replicas, the front-end wouldn't actually talk to their
			IP addresses, but to a VIP(private IP inside the swarm virtual network) that swarm puts in front of all
			services. It ensures that the load is distributes amongst all the tasks of the service.
			This is about one service talking to another inside the same virtual network.
		2. External traffic coming into the swarm can choose to hit any of the nodes.
			Any of the worker nodes are going to have that public port open and listening for that container's traffic.
			Then it will re-rout it to the proper container based on it's load balancer.

		=> When you are deploying containers in a swarm, you are not supposed to have to care about what server it's on.
		Because that might move(failover) and you certainly don't want to change the firewall or DNS settings.
		The routing mesh solves a lot of those problems by allowing our drupal site on port 80 to be accessible from 
		any node in the swarm. And in the background, it's routing those packet from that node to the proper container.
		If it's on a different node, it will rout it over the virtual network. It it's on the same node, it will just
		re-rout it to the port of that container.
		All out of the box.

Diagram example 1:
You create a service "my-web" with 3 replicas on 3 nodes in "my-network" overlay networks.
In my-network, it's gonna create a VIP mapped to the DNS name(which is same as service name)
DNS name: "my-web" with VIP: 10.0.9.2 in my-network
	- node-1: container: mt-web.1 10.0.9.3
	- node-2: container: mt-web.1 10.0.9.4
	- node-3: container: mt-web.1 10.0.9.5

Other containers in my-network talk to the service with the DNS name "my-web"
	and VIP load balances the traffic amongst all the tasks in the services

This isn't actually DNS round robin, slightly a different config(can be enabled that).
Benefits of VIP over round robin is that many times the DNS cache prevent us from properly distributing the load.
VIP is what you would have if you bought a dedicated hardware load balancer.

Diagram example 2 - external traffic coming in:

Ingress network:
One services with 2 replicas on 3 nodes
	- node-1:
		- external IP address(given by digital ocean): 192.168.99.100
			- build-in swarm load balancer on this public IP with published port 8080 (cause `-p`) and "my-web"
		- task(container): my-web.1 - 10.0.0.1:80
	- node-2:
		- external IP address(given by digital ocean): 192.168.99.101
			- build-in swarm load balancer on this public IP with published port 8080 (cause `-p`) and "my-web"
		- task(container): my-web.1 - 10.0.0.2:80
	- node-3:
		- external IP address(given by digital ocean): 192.168.99.102
			- build-in swarm load balancer on this public IP with published port 8080 (cause `-p`) and "my-web"

Example in action:
`$ docker service create --name search --replicas 3 -o 9200:9200 elasticsearch:2`
`$ docker service ps`
`$ curl localhost:9200`	-> do repeatedly to see 3 different names - VIP load balancing across the three task


Some notes:
1. This is a stateless load balancer.
	Like if your app uses session cookies or it expects consistent container to be talking to a consistent client,
		then you may need to add some other things to solve that problem.
2. This LB is at OSI layer 3 (TCP) and operates on IP/port layer, not layer 4 (DNS)
	If you want to run multiple websites on the same port on the same swarm, you will need another piece of the puzzle.

Luckily these are common things, so several options:
	1. Nginx with HAProxy LB proxy which sits in front of the routing mesh and
		acts as a stateful LB and can do caching and lots of other things
	2. Docker Enterprise edition(payed) has UCP or Docker Data Center which comes with an L4 web proxy
		allows you to throw DNS names in swarm services web config.


Swarm mode routing mesh (Docker Docs): https://docs.docker.com/engine/swarm/ingress/



77 > Assignment: create a multi-service multi-node web app

course-git-repo:swarm-app-1 directory

78 > Assignment answers

The `$ docker container create` `-v` options doesn't work with the `$ docker service create` due to it's limitations.
New improved format: `--mount type=volume,source=db-data,target=/var/lib/postgresql/data`



79 > Swarm stacks and production grade compose

Docker 1.13 added stacks - basically compose files for production production swarm.
	Includes services, networks and volumes, the stack creates/manages everything.
	We can specify external things(to use the already existing ones).
	New `deploy:` key to mention swarm specific stuff.

You can do `$ docker stack deploy`.

It can't do `build:` cause ideally, building shouldn't happen on production swarm. Your CI system should do it.
Compose ignores `deploy:` and Swarm ignores `build:`.
	=> you don't have to change the file. Same file can work for both the development and production deployment sites.

You don't need the `docker-compose` CLI binary(the non-production sys-admin/dev helper tool).
The swarm i.e. the Docker engine directly deal with the files. Stack file version 3/3.1/more is must

So, the stack includes multiple services, overlay networks and volumes. (It can also control secrets).
	Stack controls all those things, puts it's name properly on the objects to distinguish them. + labels & metadata.

A stack is only for one swarm. It can't do many swarms.

Last assignment could be a stack file!
course-git-repo:swarm-stack-1/example-voting-app-stack.yml quick go-through:
There's the new `deploy:` key:
```
version: "3.9"
services:
  ...
  redis:
    ...
    deploy:
      replicas: 1
      update_config:			# What happens when you update stack which updates the service, how you want to roll it out
        parallelism: 2			# 2 at a time(could be 1 at at time or all at at time)
        delay: 10s				# delay between them
      restart_policy:
        condition: on-failure	# auto restart the container if it fails
  ...
  db:
    ...
    deploy:
      placement:
        constraints: [node.role == manager]		# put it only on a manager node
  ...
  worker:
    ...
    deploy:
      mode: replicated
      replicas: 1
      labels: [APP=VOTING]						# can assign labels to anything, even nodes
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3							# if restart fails, it won't try more than 3 times
        window: 120s
      placement:
        constraints: [node.role == manager]
```

Deploying the stack:

`$ docker stack deploy -c swarm-stack-1/example-voting-app-stack.yml voteapp`

`$ docker stack ls`
`$ docker stack services <stack-name>` -> shows services with their replicas
`$ docker stack ps <stack-name>` -> shows tasks with task IDs(not container IDs, they are different IDs)

The stack file has a visualizer services listening on 8080. Shows all services running on nodes in UI in browser!

Updating the services using commands is kind of an anti-pattern as the changes won't be applied next time.
Instead, always change the file(the source of truth) and do `$ docker stack deploy <same-stack-name>`.
	It automatically recognizes the existing stack and just updates it.


Swarm vs Compose: https://github.com/BretFisher/ama/discussions/146


80 > Secrets storage for swarm - protecting your environment variables

Easiest secure solution for storing secrets in swarm.
Encrypted on disk, encrypted in transit
Supports generic string/binary content up to 500Kb.
Doesn't require apps to be rewritten(like, to talk to some web service etc.).

As of 1.13.0 the Raft DB is encrypted by default. Only stored on manager node disks. They are the only ones having the keys.
Default manager & worker "control plane" is TLS + Mutual Auth through which the secrets are brought down to the container.

We store secrets in swarm and assign them to services to tell it who is allowed to access them.

To apps, they look like files, but they are in memory only(using ramfs file system) not on disks:
One `/run/secrets/<secret_name>` or `/run/secrets/<secret_alias>` file per secret.
File name is the key and the contents are the value. Can assign aliases(multiple names) to same secret.

The keys depend on the swarm's encryption feature.
If you don't have swarm enabled and are using docker-compose, which should never be used in production,
	it "fakes" the security by storing the secrets in plain text files.
Keys are a swarm-only feature.


81 > Using secrets in swarm services

course-git-repo:secrets-sample-1/ contains a file containing a secret.
Two ways:
1. File
	`$ docker secret create <secret-name> <file>`
2. Command-line
	`$ echo "<password>" | docker secret create <secret-name> -`

Don't store the secrets in the files on the remote hosts(use local docker CLI to do this).
And beware of shell history in option 2!

Now only the containers in the services to which you assign these keys can access them.

Example:
`$ docker secret create psql_user <file>`
`$ echo "<password>" | docker secret create psql_pass -`

The app doesn't have any way to know how to access the secrets. So we usually pass environment variables with secret names.
Postgres has standard ones:

`$ docker service create --name psql --secret psql_user --secret psql_pass \
	-e POSTGRES_USER_FILE=/run/secrets/psql_user \
	-e POSTGRES_PASSWORD_FILE=/run/secrets/psql_pass \
	postgres`

If this wouldn't work, if the DB wouldn't get the password, the container would keep failing and swarm would keep
	creating new containers.

We could do `$ docker service update --secret-rm` (there's also `--secret-add`), but that would redeploy the container.
	Because secrets are part of the immutable design of services. If anything in the container has to change for the
	service, the service won't go in and change it inside the container. It will actually stop the container and re-deploy
	a new one.

	So we will need some other plan to update the DB passwords(discussed later).


`$ docker secret ls`
`$ docker secret rm ...` to remove them.

Docker docs: https://docs.docker.com/engine/swarm/secrets/


82 > Using secrets with swarm stacks

course-git-repo:secrets-sample-2/ contains a compose file and two files containing secrets.

This needs the compose file version to be at least 3.1.
```
version: "3.9"

services:
  psql:
    image: postgres
    secrets:
      - psql_user
      - psql_password
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/psql_password
      POSTGRES_USER_FILE: /run/secrets/psql_user

secrets:
  psql_user:
    file: ./psql_user.txt
  psql_password:
    file: ./psql_password.txt
```

OR you can also have the secrets pre-created and instead of `file:`, you can mention "external" to use existing ones.

Above is the short-form, there's also a long-form that allows us to define permissions and users that are allowed
	to access the keys using the standard Linux mode and user ID syntaxes.

`$ docker stack rm <stack>` also automatically removes the secrets.

Secrets in compose file: https://docs.docker.com/compose/compose-file/09-secrets/


83 > Assignment

Use compose-assignment-2.
Use the official image drupal:8.2 and remove `build:`.
Create secret with CLI and add `external:` to use them.
You'll need to pass environment variable POSTGRES_PASSWORD_FILE.

84 > Assignment answer



[10] Swarm App Lifecycle

85 > Secrets with local docker-compose

Secrets need Swarm. But docker allows the file based secrets to work with the non-production dev tool docker-compose in
a non-secure way, so that you can use the same compose(stack) file for development.
It just bind mounts the actual file from your hard drive into the container.

course-git-repo:secrets-sample-2/

Doesn't work with external secrets. There, you will have to have separate compose file for development


res/dm-s09-commands.txt
res/s09-swarm-app-lifecycle-slides.pdf


86 > Full app lifecycle: dev, build, and deploy with a single compose design

Sometimes, you might wanna use multiple compose files(don't have to).

course-git-repo:swarm-stack-3/ example: we have:
	- Dockerfile
	- docker-compose.yml			-> base compose file, sets the defaults that are common across all environments
	- docker-compose.override.yml	-> (standard name) automatically brought in when `$ docker compose up`
	- docker-compose.prod.yml		-> non-standard name `-f` is required
	- docker-compose.test.yml		-> again, `-f` is required, uses fake password, no volumes(it's just for test cases)

The docker-compose.override.yml	is for local development. Using standard name as during development, the hand typed
commands should be as small as possible. Uses file based secrets (cause it's local remember? Swarm might not be there).

The docker-compose.test.yml is just for CI environment for running the test cases. Has `build: .` an image on every commit.
Publishes ports used for testing purposes. It uses fake password. But don't need to define any other volumes.
Instead, bind mounts a sample-db volume(which may come from a git repo or FTP download)
	=> same data for every CI test suite run

The docker-compose.prod.yml contains all the normal production concerns: data volumes etc.
Uses external secrets, cause we're going to have put the secrets in already via CLI
We won't have the docker-compose CLI on the production server, instead we will you a `$ docker compose config` command.

We do:
	- for dev: `$ docker compose up` -> automatically takes docker-compose.yml + docker-compose.override.yml combined
	- for CI: `$ docker compose up -f docker-compose.yml -f docker-compose.test.yml up` -> base first
	- for prod: `$ docker compose up -f docker-compose.yml -f docker-compose.prod.yml config > output.yml`
		-> combines both, we do this somewhere in our CI solution and then we use the output.yml to create/update our stack.

There's new compose `extents:` option to override files. Not released as of the course video recording.


Using multiple compose files: https://docs.docker.com/compose/multiple-compose-files/extends/#multiple-compose-files
Using compose files in production (Docker docs): https://docs.docker.com/compose/production/


87 > Service updates: changing things in flight

Rolling replacement for tasks/containers in services is provided.
Limits downtime. Though you need to test early & test often.
Unless you are updating a label or some other metadata, it will mostly replace the containers.
Creation options usually change, adding "-add" or "-rm" to them.
Includes rollback and healthcheck options. Change if their default values aren't ideal for you.
Now `$ docker service scale web=4` & `$ docker service rollback web` are separate commands instead of options.
Remember: stack deploy to existing one(with an edited compose file) is an update.

Examples:
1. Change the image of a service:
`$ docker service update --image myapp:1.2.1 <service>` -> every time you update your app and build a new image

2. Multiple in one: add an environment variable and remove a published port:
`$ ... update --env-add NODE_ENV=production --publish-rm 8080 ...`

3. Change number of replicas in two services:
`$ docker service scale web=8 api=6` -> multiple in one is the advantage over the update's option

4. Change published port: you have to remove and add:
`$ ... update --publish-rm 8080 --publish-add 9090:80 ...`

Tip: If you change a lot of things around, if you have a lot of containers in the swarm, you may find that they aren't
really evened out. Some nodes may have a lot of containers while others have less. Swarm won't auto balance
You can force an update even without changing anything in the service. Then it will reissue tasks and will pick the least
used nodes -- a form of rebalancing.

`$ docker service update --force <service>`

It will use the scheduler's default of looking for nodes with the leasts number of containers/resources used.


Service update commands Docker docs: https://docs.docker.com/reference/cli/docker/service/update/


88 > Healthcheck in docker files

New in 1.12. Supports docker file, compose file, docker run & swarm services.
Recommended in production.
Docker engine will `exec` that command inside container with simple 0 or 1 return.
Only 3 states:
	starting -> first 30 seconds, hasn't run the healthcheck command yet
	healthy -> if returns 0, then the command is run every 30 seconds
	unhealthy -> if it ever returns 1

Much better option than "binary still running?" !
Not a replacement for 3rd party monitoring solutions. It won't give you graphs or status over time etc. stuff.
Not super advanced but a basic level of health check. Example, for nginx it might check localhost of the root index file.
	If it returns 200/300, then it's healthy, but 400/500 etc. then it's unhealthy.

Where do we see it?
	`$ docker container ls`
	`$ docker container inspect` -> history of last 5 checks

`docker run` doesn't automatically take any action on unhealthy containers.
But swarm services replace those containers. Updates also consider the healthcheck as a part of the readyness.

Example: adding a healthcheck in a container whose image doesn't have it.
```
$ docker run \
	--health-cmd="curl -f localhost:9200/_cluster/health || false" \
	--health-interval=5s \
	--health-retries=3 \			# retries before considering it unhealthy
	--health-timeout=2s \
	--health-start-period=15s \		# additional to first 30 seconds for slow starting apps
	elasticsearch:2
```
Exit code must be 0 or 1(not non-zero), hence the `false` at the end for commands that return non-zero.

Dockerfile health checks:

```
HEALTHCHECK -timeout=3s -interval=30s -retries=3 \			# options come before the CMD
	CMD curl -f http://localhost/ || exit 1					# `exit 1` works same as `false`.
```

Different apps support different health check tools:
- PHP has localhost/ping that can be curled (check good PHP defaults in the resources).
- postgres has abuilt-in tool: `$ pg_isready -U postgres`

Compose file health checks:

```
services:
  web:
    ...
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 1m			# requires compose version 3.4/above
```

The default 30 seconds + the start-period provided will be seen added in `$ docker service create`.
It will be in "starting" for that period and then go in "running" to ensure the container health.


Resources:
PHP Laravel good defaults with docker: https://github.com/BretFisher/php-docker-good-defaults
Dockerfiles healthcheck Docker docs: https://docs.docker.com/reference/dockerfile/#healthcheck
Compose files healthcheck Docker docs: https://docs.docker.com/compose/compose-file/#healthcheck


89 > Swarm mastery course
Checkout out the last lecture of this course for it.



[11] Container Registries: Image Storage and Distribution

90 > Docker Hub: digging deeper

Image registries is a required part of the container echosystem. A place to store the images off your dev workstation is
required. It can be on the internet or private.

Docker Hub is an image registry + lightweight building(which is extra, not part of a registry).

Docker Hub > Create > Create Automated Build
	-> choose GitHub or BitBucket -> choose repository -> it automatically builds the image on every commit
	Docker Hub is notified when some event happens
	Docker Hub rep > build settings to customize(choose branch etc.) and
	Repo links -> if you are building on Docker Hub and using the `FROM` statement to pull from some other image,
		then you want to setup repo link so any time that software changes, your image get's auto rebuilt.
	Build trigger -> gives you an endpoint. POST on it will trigger a build.


Web hooks are for Docker Hub to notify something else.

Docker Hub: https://hub.docker.com/
res/dm-s10-commands.txt
res/s10-container-registries-slides.pdf


91 > Understanding docker registry

Running private registry:
It's a part of the docker/distribution code repo on GitHub.
This is V2(rewritten in Go). There's also a legacy old version.
It's bare-bones, no UI. It does support certificate and basic auth(no RBAC) OOTB, but not enabled by default.
Without any tools on top of it, it's useful in small teams.
At it's core, it's just a web API and a storage system.
Comes with a lot of storage drivers, allows you to put the back-end store on: local/S3/Azure/Alibaba/GCP/OpenStack Swift

You can also use the `--registry-mirror` option to mirror the Docker Hub. It caches the images near your metal.
So, download the images from the Docker Hub once and cache them for all machines to pull.


Use registry as a "mirror" of Docker Hub: https://docs.docker.com/docker-hub/mirror/
Registry docs: https://distribution.github.io/distribution/
Registry garbage collection: https://distribution.github.io/distribution/about/garbage-collection/


92 > Run a private docker registry

HTTP server listening on port 5000 by default.

Docker doesn't talk to registries without TLS, except for localhost(127.0.0.0/8).
	For remote self-signed TLS, enable "insecure-registry" in engine.

For the demo, we will:
	1. Re-tag an existing image and push to our new registry.
	2. Remove the image from local cache and pull from the new registry.
	3. Recreate the registry using bind-mount and see hot it stores the data.


Let's run the registry:
`$ docker container run -d -p 5000:5000 --name registry registry`

The account-name/repository-name format is for Docker Hub. For any other registry, we need to label it with the host.

`$ docker pull hello-world`				- pulling our sample image
`$ docker container run hello-world`	- just prints some text to validate that the container ran

To push it to a local registry, we need to give it another tag.

`$ docker tag hello-world 127.0.0.1:5000/hello-world`
	- This is equivalent of making our own official image at the root of our local registry.

`$ docker push 127.0.0.1:5000/hello-world` - it will see the IP/port and push to the local registry.

Let's remove the images from the local cache
`$ docker image remove hello-world`
`$ docker image remove 127.0.0.1:5000/hello-world`

`$ docker image ls` -> we don't have it anymore.

`$ docker image pull 127.0.0.1:5000/hello-world` -> pulls from the local registry.


You should actually use a volume to store the data. Let's do that.

Removing the first one:
`$ docker container kill registry`
`$ docker container rm registry`

And create again:
`$ docker container run -d -p 5000:5000 --name registry -v $(pwd)/registry-data:/var/lib/registry registry`
`$ ls registry-data` -> empty.

This is technically a new registry, pushing the hello-world image will push to it.
`$ docker push 127.0.0.1:5000/hello-world` - pushes to the new local registry.
`$ ls registry-data` -> not empty.


93 > Assignment: secure docker registry with TLS and auth

TODO

The default registry is bare-bones and anyone can pull/push. You need TLS to use it over HTTPS and the add some basic auth.

"These aren't hard to learn, but do require some commands. You can learn the basics by creating a self-signed cert
for HTTPS, and then enabling `htpasswd` auth to add users."

For this assignment, use play-with-docker.
Do Part 2 & 3 of "Docker Registry for Linux": https://training.play-with-docker.com/linux-registry-part2/
	You can use their text to do the assignment on your local machine, or see their part 1 to run in on their infra
		using their web based interface to a real docker engine and learn how the "PWD" works:
		https://training.play-with-docker.com/linux-registry-part1/

For extra credit lags, look through their growing list: https://training.play-with-docker.com/


94 > Using docker registry with swarm

Works the same way as the localhost. The routing mesh allows all the nodes to talk to the registry as if it's local.
Don't even have to enable "insecure-registry".
And you need to remember to use a volume to persist the images.

Demo on play-with-docker using 5-manager-node swarm:
`$ docker service create --name registry -p 5000:5000 registry`
`$ docker service ps registry` -> running

Play-with-docker gives a URL to published ports.
On browser, You can go to: <URL>/v2/_catalog -> empty JSON array: `{"repositories":[]}` (this also work on local registries)

`$ docker pull hello-world`
`$ docker tag hello-world 127.0.0.1:5000/hello-world`
`$ docker push 127.0.0.1:5000/hello-world`

Browser : <URL>/v2/_catalog -> `{"repositories":["hello-world"]}`

Do the same thing with nginx:

`$ docker pull nginx`
`$ docker tag nginx 127.0.0.1:5000/nginx`
`$ docker push 127.0.0.1:5000/nginx`

Browser : <URL>/v2/_catalog -> `{"repositories":["hello-world", "nginx"]}`

`$ docker service create --name nginx -p 80:80 --replicas 5 --detach=false 127.0.0.1:5000/nginx`

All the nodes from the swarm must be able to pull their images from some central registry. They can't share with each other.
Our example works because our registry is running as a service here and all nodes know how to access 127.0.0.1
and the routing mesh steers them to the correct container.

If you don't have to run your own registry, you can just use one of the hosted ones on the internet.
They are redundant, have monitoring, have proper storage management and mostly free for open source or reasonably priced.
Examples: AWS, Quay, Docker Hub. Try them first and use your own registry if you have some offline or security requirements.


95 > Third party image registries

Apart from Docker Hub, Docker Enterprise Edition DTR(Docker Trusted Registry) and Docker Registry, there are many.

Quay.io is popular.
Plus, all 3 clouds have their own services.
Plus, self hosted options:
	Docker EE - https://www.docker.com/#/container_management
	Quay Enterprise - https://quay.io/plans/?tab=enterprise
	GitLab Container Registry - https://about.gitlab.com/blog/2016/05/23/gitlab-container-registry/

Larger list here: https://github.com/veggiemonk/awesome-docker#hosting-images-registries



[12] Docker In Production

96 > Bret's DockerCon 2017 talk on Docker and Swarm in production

TODO

"Still relevant today".
https://www.youtube.com/watch?v=V4f_sHTzvCI
	There's a newer version? 2018?

res/road-to-production.pdf

97 > The future of swarm

K8s overshadowing swarm?
But swarm is still getting updates?
Discussion: https://www.youtube.com/watch?v=L5N43aQQArw&t=223s

98 > Swarm Raft quorum and recovery

YouTube: "Everything You Thought You Already Knew About Orchestration": https://www.youtube.com/watch?v=Qsv-q8WbIZY



[12] The What and Why of Kubernetes

99 > Intro to Kubernetes
Nothing mych in this chapter.

res/slides-the-what-and-why+of-kubernetes.pdf


100 > What is Kubernetes

What's an container orchestrator?
Takes your containers, that you ask it to run and a series of nodes
	and decides how to run those containers across those nodes.
	It makes many servers act like one.

K8s was released in 2015 by Google and is now maintained by open source community including Google.

Kubernetes essentially is a set of APIs that run on apps in containers to manage a set of servers
and execution your containers on docker by default. It can run other container runtimes that aren't docker like containerd.
It provides you a set of API/CLI to manage containers across servers.

Tons of options to get it:
	1. Cloud vendors - provide kubernetes as a service
	2. Many vendors make a "distribution" of it.. kind of like Linux distros.

Resources:
History of kubernetes: https://en.wikipedia.org/wiki/Kubernetes
Kubernetes home page: https://kubernetes.io/



101 > Why Kubernetes

Chapter 69 "Swarm mode" for why we need orchestration.
Important to understand whether you need orchestration or not and what benefits of it.

Some people still use single node and features of their platform, like elastic load balancer etc.
And they have such a refined system that they don't see a lot of benefits in completely changing all that to orchestration.
But orchestration is really the way the industry seems to be going.

Formula to decide whether orchestration is needed: Servers + change rate = benefits of orchestration.
	Number of servers needed for a particular environment and the change rate of the app or the environment itself.
		Multiplication of those two = benefits of orchestration.

	"Orchestration is designed to automate change and monitor the state of things and
		ensure everything is in the state you expect it to be in."

	"If you are changing your app once a month or less, then the orchestration effort may be unnecessary,
		especially if you are a solo developer or just a very small team."
		That's where things like Elastic Bean Stalks or Heroku app deployment platforms shine as alternatives
			to doing your own orchestration.

And how to decide which one you want?
There are 4-5 major platforms including the clouds:
Top 2: Swarm & Kubernetes, then there are cloud platforms like AWS ECS
	and traditional ones like Cloud Foundry or Mesos and Marathon.

But if you are concerned about running the containers on-prem and in cloud or multi-cloud, you may not want to go with
	the cloud specific offerings like ECS. Cause those were around before K8s was. That's Amazons kind of legacy solution.
	And there's vendor lock-in.
	In that case, Swarm and Kubernetes ate the two options.

If Kubernetes is decided, then which distribution?
Do you want a cloud managed or a self-managed solution?
Self-managed solutions are vendor's product which you will install on on the servers,
	like Docker Enterprise, Rancher, OpenShift, Canonical, VMWare PKS.
	Use the certified distributions in the link below so you can change from one vendor to another.

Another option: use raw GitHub upstream version of kubernetes.
But you probably don't need it. Kubernetes needs a lot of things added to really make it easy to use OOTB.
So you will probably want the extra vendor solutions like custom auth, custom web administration, custom networking etc.
But it's great for learning and recommended to start with as the way to learn K8s.

Resources:
List of certified kubernetes distributions: https://kubernetes.io/partners/#conformance


102 > Kubernetes vs Swarm

Swarm is easy, but doesn't solve all the problems.
Kubernetes has much more functionality, flexibility, & customizations and can solve more problems in more ways
	and generally has wider adaption and support.

Swarm advantages:
Single vendor container platform - Docker & swarm are both from the Docker.
Easy orchestration to deploy/manage yourself.
Kind of has 20% of Kubernetes features that solve 80% of the problems.
Runs anywhere docker does: local, cloud, datacenter, ARM, Windows, 32-bit - longest & widely supported orchestrator.
Secure by default.
Easier to troubleshoot.

Basically, good for starting out with 2-3 people, unless you know you absolutely need kubernetes.

Kubernetes advantages:
Widest cloud and vendor support - kind of a zeitgeist.
	Infra vendors too have support for it including VMWare, Redhat.
Widest adaption.
Widest set of supported use cases.
"No one ever got fired for buying IBM machines" - basically, if you couldn't make a decision,
	you would just go the safe way(with IBM) and you would never get fired for that because it was trusted at the time.
	- Picking a solution isn't 100% rational
	- "Trendy" will benefit your career
	- CIO/CTO/higher management checkbox even if you don't have any technical requirement


[14] Kubernetes architecture and install

103 > Section intro
Nothing much.

Resources:
res/slides-kubernetes-architecture-and-install.pdf
res/section-14-commands-and-links.pdf


104 > Kubernetes architecture technology

Terms:
Kubernetes or K8s: the whole orchestration system.
Kubectl "cube control": CLI to configure k8s.
Node: Single server in kubernetes cluster. Workers are generally called nodes.
Kubelet: container that will run a small, little agent on each node to allow that node to talk back to k8s master.
	Since Swarm was builtin to Docker, it didn't need a separate agent. It was all built into the docker engine.
Control plane[aka master]: set of containers that manage the cluster. They run individual thing taking the Unix
	principle of "do one thing and do it well". Includes: API server, scheduler, DB back-end(etcd), controller manager, etc.
	Just like Swarm, odd number of masters are needed. It uses the same Raft protocol.

	In Swarm, "Control plane" was the protocol that over the network that were allowing master and workers to talk securely.
	But here, the set of master nodes is called the control plane.

You generally keep your apps to the nodes and the k8s management system to the masters.

K8s supports docker as well as other containers like containerd or crio.

Resources:
Kubernetes components: https://kubernetes.io/docs/concepts/overview/components/#master-components

Each master has multiple containers to manage k8s:
etcd -> key-value storage, uses the same Raft protocol, so need odd number. Can also be installed without k8s.
API -> for us to talk to the cluster and issue orders.
Scheduler -> decides how and where your containers are placed on the nodes in objects called pods.
Controller manager -> looks at the state of the whole cluster and everything that's running in it using API. Takes the
	orders given to it(or the spec) and determines the difference between the order and what's actually going on.
Core DNS
+ there can be more containers running depending on add-ons you get like networking, storage, or web etc.

Each node has following running:
the agent called kubelet is running.
kube-proxy is needed to control the networking.



105 > Kubernetes local install

Many ways to install.. will focus on the easiest ones for learning.
If Docker Desktop is installed, just enable K8s in settings. Sets up everything in it's Linux VM.
For people with Docker toolbox, MiniKube has similar experience(doesn't require toolbox) - uses VirtualBox to create VMs.
	- Install with minikube-installer.exe and do `$ minikube start`
	- requires VirtualBox to be installed
	- Nedds kubectl to be installed separately
If you have Linux desktop or VM: MicroK8s
	- uses snap instead of apt/yum, so install that first
	- `$ microk8s.` + Tab to see all it's commands
	- installs microk8s.kubectl which you can just alias to kubectl: `alias kubectl=microk8s.kubectl`

In browser:
play-with-k8s.com - requires you to install the cluster and set it up.
kadacoda.com - provides the setup for you.

Resources:
Minikube download: https://github.com/kubernetes/minikube/releases/
microk8s for Linux hosts: https://github.com/canonical/microk8s
Install kubectl on Windows without Docker Desktop: https://kubernetes.io/docs/tasks/tools/#install-kubectl-on-windows
play-with-k8s: https://play-with-k8s.com/
killercode k8s in your browser: https://killercoda.com/


106 > Kubernetes container abstractions

Pod: basic unit of deployment. We don't technically deploy containers, we deploy pods
	They are 1/more containers running together on a node.

Controllers: control(create/upload) the pods. Technically, you can directly deploy pods, but you won't wanna do that.
	Controller continually sits there & validates whether or not, what's going on in the k8s is what you've asked it to do.
	It's a differencing engine that has different types like Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob
		etc. and even 3rd party containers.

Service: endpoint you give to a set of pods, a persistent endpoint in the cluster so that everything else can access
	that set of pods at a specific DNS names and port. Unlike swarm services, it has nothing to do with the deployment.

Namespace: just a filter on your view at the command line, so that you only see the things you care about at the moment.
	Example, when using docker desktop, it defaults to the default namespace and filters out all the system containers
		running k8s in the background, cause normally you don't want to see those when running kubectl. You can change
		the namespace to view those.

Resources:
Pod overview: https://kubernetes.io/docs/concepts/workloads/pods/
Service docs: https://kubernetes.io/docs/concepts/services-networking/service/
Namespace docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/


[15] Your first pods

107 > kubectl run, create and apply

kubectl is also evolving.

Resource:
res/slides-your-first-pods.pdf
res/section-107-commands-and-links.pdf
kubectl reference docs: https://kubernetes.io/docs/reference/kubectl/


108 > Your first pod with `$ kubectl run`

`$ kubectl version` - are we working? Is the API working?

`$ kubectl run my-nginx --image nginx` - for creating a single pod(unlikely to be used in production), name is required
-> "pod/my-nginx created" -> doesn't mean the pod is running the container, just means that the request to create the
	container is received and stored the entry in the etcd DB.

`$ kubectl get pods`

`$ kubectl get all` -> shows the common standard objects. Not all(there are too many). This is per namespace.

Pod is not a real things. It's just an idea of a resource type that wraps around 1/more containers that share
	the same IP address and the same deployment mechanism.
	=> all containers in a pod are deployed together in the same node and have access to each other over localhost.

You create pods (via CLI, YAML, API) and k8s creates the containers inside them.
	You request for the container - which goes to the control plane which instructs the kubelet.
	The kubelet tells the container runtime(docker/containerd/crio) to create the containers for you.

Resource:
kubectl cheat sheet: https://kubernetes.io/docs/reference/kubectl/quick-reference/
kubectl for Docker users: https://kubernetes.io/docs/reference/kubectl/docker-cli-to-kubectl/

109 > Cheat sheets for kubectl

Entire website full of cheat sheets(any topic): https://cheatography.com/
Kubernetes concepts: https://cheatography.com/gauravpandey44/cheat-sheets/kubernetes-k8s/
Some commands and resources: https://cheatography.com/deleted-44122/cheat-sheets/kubectl/


110 > Your first Deployment with `$ kubectl create deployment`

Kubernetes Deployment - the most popular resource, roughly equivalent to swarm service.
	Allows you to create 1/more pods with same config across many nodes.
	For pods with no specific reason for existing(unlike jobs, cron jobs, daemon sets etc.) like your nodejs app etc.

`deploy`, `deployment`, `deployments` - all work

`$ kubectl create deployment my-nginx --image nginx` - likely to be used in production
	same name works because the previous pod with the same name is a resource of type pod
		and this is a resources of type deployment.

`$ kubectl get pods`
`$ kubectl get all`

Your request goes to the API into the etcd DB. The controller manager's job is to look at all the different resource types.
If it see a new resource type, hopefully, a controller is assigned to that resource type.
In this case, the resources types of pod & deployment are builtin(to every k8s).
The controller manager sees the new deployment resource. The controllers manger itself controls deployment resources (it has
that controller builtin). The job of the deployment resources is to do 1 thing: to create a replica set. It doesn't
directly create pods. It creates a replicaset for each new deployment type and version to support the rolling upgrade,
blue-green deployments. Then the replicaset controller(again part of the controller manager) will see the replica set entry
and create the required pods entries in the DB. The scheduler will notice the pods scheduled to a node. Then it's assigned
to a kubelet which instructs the container runtime which then creates the container. All happens in seconds.

Hence the pod name displayed is <deployment-name>-<replicaset-name>-<pod-name> to guarantee the unique name.

`$ kubectl delete pod my-nginx`
`$ kubectl delete deployment my-nginx`

Resources:
kubectl create deployment docs: https://kubernetes.io/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/
Deployment docs: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/


111 > Scaling replicasets

For HA, load balancing, or fault tolerance.

`$ kubectl create deployment my-apache --image httpd`


`$ kubectl scale deploy my-apache --replicas 2`
Or
`$ kubectl scale deploy/my-apache --replicas 2`

Creates one more(total 2) in the same replicaset as only the replica count is changed.
If we would change say image name, or image tag it would create a new replicaset. Both replicasets would run side by side
	for a period of time, while the old one goes away.
So just the replicaset will be edited to 2. The scheduler in the control plane will see the new pod request,
	find anode with available resources, schedule the pod for that node and the kubelet on that node will...

Resources:
ReplicaSets docs: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/

112 > Changes to kubectl run

Since version 1.18:
`$ kubectl run` has not reduced complexity and creates a single pod, like `$ docker run` which creates a single container.
Other resource creation is moved to the `create` command.

There's also course update project: https://github.com/users/BretFisher/projects/3/views/1



[16] Inspecting Kubernetes Resources

113 > Section Intro

Nothing much.

Resources:
res/slides-inspecting-kubernetes-resources.pdf
res/commands-and-links-inspecting-kubernetes-resources.pdf


114 > Inspecting resources with `get`

`$ kubectl get all -o wide` -> more details
`$ kubectl get all -o yaml` -> lot of details with defaults
`$ kubectl get --help` -> output options including json, go-templates and more

labels & selectors are useful for finding the related resources, especially in large projects

`$ kubectl get deployment/my-apache -o yaml`


115 > Inspecting resources with `describe`

`get` is great and focuses on giving you all the info of 1 resource.
When we create resources, like Deployment, there are related resources like ReplicaSets and pods.

Sometimes it's more helpful to get a comprehensive view of all the things happening and a concise view of the imp details.
Most commonly used fist touch command `describe`:
`$ kubectl describe deployment/my-apache`
`$ kubectl describe pod/my-apache-xxxx-yyyy`
	Combines key resource details and related resource details into a single human readable output.

Typical workflow of high to low level details:

`$ kubectl get nodes`
`$ kubectl describe node/<node-name>`


116 > Watching resources

Commands to get the real-time info.
Remember the Unix `watch` command? Kubernetes has similar functionality builtin with `-w` or `--watch` option.

`$ kubectl get pods -w` -> shows all the events occurring with pods in real-time.
	For example, if we delete a pod directly with `$ kubectl delete pod <pod>`,
		then we will see the event logs of new pod coming and the old one going away.

`$ kubectl get events --watch-only` -> shows all events happening in real-time. Bit more helpful entries.
`$ kubectl get events` -> same, but also shows previous entries.

These events are stored in the etcd DB. But this is not a kubernetes log, as there are a lot of things it doesn't have.
	Like anything outside of the awareness of the kubernetes API, container logs.
	And This is not meant for long term storage or searching etc.
	But it's helpful to figure out what's going on when things are misbehaving.


117 > Container logs in kubernetes

The container logs are stored, by default, on each node with the container runtime.
`$ kubectl logs` tell the kubelets on each node wherever the pods are, to start sending the logs to the API,
	that can them be sent to you.
It's for troubleshooting, when you are on a very small platform and maybe don't have a centralized logging solution.

`$ kubectl logs deploy/my-apache` -> logs of a deployment? No.
	It just chooses a random pod within it and shows it's logs. It the pod has multiple containers, the first one is chose.
	I.e. the first one listed in YAML > spec > containers list

`$ kubectl logs deploy/my-apache --follow --tail 1` -> follow only the new log entries with 1 historical entry

`$ kubectl logs pod/my-apache-xxxx-yyyy -c httpd` -> logs of a specific container in a specific pod

`$ kubectl logs pod/my-apache-xxxx-yyyy --all-containers=true` -> logs of all containers in a specific pod merged
	Difficult to watch as there's not date/time and the order is not guaranteed.

But we have 2 pods. What if we didn't know which pod is misbehaving? We can use label search to watch those logs.
`$ kubectl describe deployment/my-apache` -> see the "Labels: <lable>" in output, example: "app=my-apache"

We can search for any pod with that label to show the logs: `$ kubectl logs -l <label>` Example:
`$ kubectl logs -l app=my-apache` -> kind of merged logs.
	Again, this works when you have few containers.
		But as you grow, you are gonna need centralized logging.

There are other tools that can help like stern on GitHub. Makes a little bit easier to find these logs.
`$ stern my-apache`
	Does things like:
		- auto collecting logs from multiple pods
		- colorizing the logs from different pods which kubernetes doesn't do

`$ kubectl delete deployment/my-apache`

Resources:
System logs (docs): https://kubernetes.io/docs/concepts/cluster-administration/system-logs/
Logging architecture (docs): https://kubernetes.io/docs/concepts/cluster-administration/logging/
Debugging running pods (docs): https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/
Loki, like Prometheus, but for logs: https://github.com/grafana/loki
ELK stack: https://www.elastic.co/elastic-stack/
Stern multi-pod log tailing: https://github.com/stern/stern


[17] Exposing Kubernetes Ports

118 > Section intro

Nothing much.

Resources:
res/slides-exposing-kubernetes-ports-with-services.pdf
res/commands-and-links-exposing-kubernetes-ports.pdf


119 > Service types

Now that you have your pods running, you will mostly want to expose them outside at some point
	i.e. allow them to accept connections in some fashion in our from outside of the cluster.

One of the ways to create a service: `$ kubectl expose`.
Remember: a service is an endpoint that's consistent so that other things inside/outside of the cluster
	might be able to access it.

Pods don't automatically get DNS names for external connectivity with an IP address. We need to create a service
	on top of that existing pod.

Multiple ways. The common one is: CoreDNS (one of the core features of the control plane) will be our DNS server
On top of that, how it gets the traffic to your service is the job of one of the following service types:
	- ClusterIP
	- NodePort
	- LoadBalancer
	- ExternalName

1. Service type "ClusterIP" (default)
Only available in the cluster => 1 set of pods talking to another set of pods.
	Doesn't need anything special(firewall rules outseid of the cluster for traffic to get to it).
Gets it's own DNS address(in CoreDNS).
Gets a VIP in that virtual IP address space inside the cluster.
	That allows other pods inthe cluster to talk to this service using it's port.
	So if you are running an nginx app on port 80, you could have the service IP also on port 80 for consistency.
	Similar to when you create a swarm service and it gets an internal IP on the load balancer
		and it runs it on the port that the app is on.

2. Service type "NodePort"

Designed for something outseide of the cluster to talk to your service through the IP address on the nodes themselves.
So, creating a NodePort, you will get a high port(like port 80 wont work) on each node that's assigned to this service.
Once you get that, you can update other things that need to know about this port.
=> anyone can connect if they can reach the node

These 2 services are always available in any kubernetes cluster: cloud, your own k8s, Docker Enterprise, Rancher etc.

3. Service type "LoadBalancer"
Mostly used in the cloud for traffic coming to the cluster from an external source.
Controls an external LB to the cluster through the k8s command line.
Essentially, it's a bunch of automation. It's going to auto create the clusterIP in NodePort so that they are available.
	And then it's gonna talk to the external system like AWS LB.

Basically, these are gonna come from the infrastructure provider
	and they will allows k8s to talk to them through some remote API and configure them for you.
	For example, when you use AWS LB, it's creating those ClusterIPs & NodePorts in the back-end but it's also telling
		the AWS LB to talk to the NodePorts.

4. Service type "ExternalName"
Used less often and has nothing to do about controlling inbound traffic to your service.
This is more about stuff in the cluster needing to talk to outside services.
Creates DNS names in CoreDNS system, so the cluster can resolve external names that you might not have control over.

One of the reasons you will use them is when your are doing migrations of things.
Like, you have a remote app outside your cluster which already has a DNS name.
But if your are moving something, say from those external services to the internal services, you might look at how you
can use ExternalName as a substitute to control the DNS inside your k8s workflow


Apart from these 4, there's another way the traffic can get inside the cluster: through ingress.
Specifically designed for HTTP traffic. Discussed later.

Resources:
Services docs: https://kubernetes.io/docs/concepts/services-networking/service/
Service types docs: https://kubernetes.io/docs/tutorials/services/


120 > MicroK8s and DNS for services

Internal cluster DNS isn't a mandatory required feature of k8s(unlike etcd, API, scheduler, controller managing, kubelets).
The standard DNS used in k8s is the CoreDNS project and is usually installed as part of your k8s distribution,
	cause you can't use k8s service resources without it. For 1 pod to talk to another, you usually need a service endpoint
	which usually needs a friendly DNS name, which needs CoreDNS.

Bret: "Services are used in every cluster I have seen, so I consider CoreDNS as a required service in any k8s".

MicroK8s tries to be as minimal as possible on a default install, so an extra step to install CoreDNS is required.
If you are using microk8s, you will need to enable CoreDNS to continue the course:
`$ microk8s enable dns`
`$ microk8s status`		`- to check the status

More info on MicroK8s:
	- home: https://coredns.io/
	- getting started: https://microk8s.io/docs/getting-started
	- command reference: https://microk8s.io/docs/command-reference


121 > Creating a ClusterIP service

ClusterIP demo using simple HTTP web server which when hit with curl, returns it's environment variables:

Watch the pods on one window:
	`$ kubectl get pods -w`

Create the deployment:
	`$ kubectl create deployment httpenv --image=bretfisher/httpenv`

Scale the deployment up:
	`$ kubectl scale deployment/httpenv --replicas=5`

Create the service:
	`$ kubectl expose deployment/httpenv --port 8888`
		-> doesn't make any changes to the pods, but creates a service(ClusterIP by default) in front of the deployment

Check the newly created service:
	`$ kubectl get service` -> there will be 2: one will be the kubernetes endpoint for API and the one we created

Remember ClusterIP is only usable inside the cluster. So we need another pod in the cluster to curl it.
Just another pod, no deployment needed:
	`$ kubectl run tmp-shell --rm -it --image bretfisher/netshoot -- bash`
		- similar options to docker, the image has a bunch of networking and Linux utilities in it

Now we have a prompt inside the pod(container).
The service name that we created becomes part of the DNS name that we created.
So similar to swarm, we can use the service name to get it with DNS:
	`$ curl httpenv:8888`

		This goes to one of the 5 pods of the deployment and we get it's environment variables,
			including hostname which is the pod name. Running it again can go to another pod.

Resources:
Using Services docs: https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/


122 > Running netshoot in kubernetes

The netshoot image contains utility for troubleshooting Linux, Docker, Kubernetes and networking. Hence the name netshoot.
It runs zsh by default, but the previous example replaced it with bash
	just to show how the `--` separates the `kubectl` options and the custom command.


123 > Creating a NodePort and LoadBalancer service

Let's expose a NodePort so we can access is via the host IP(outside the cluster).
`$ kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort`
`$ kubectl get services` -> shows new service with ports something like: 8888:31078/TCP
	This is opposite of the swarm format. The port on the left is inside the container.
	The port on the right is the port on your nodes exposed to the outside world. It came from a default range of NodePorts
	that's preset inside the cluster: 30000-32767 (configurable)

On localhost:
`$ curl localhost:31078` -> request from the outside: returns response
	on Mac/Windows, Docker Desktop provides a convenience layer(called vpnkit) to allow us to directly use this command.

NodePort internally creates a ClusterIP. It takes the connection coming into the high port
	and redirects it to the ClusterIP for that service. Configurable: can avoid the ClusterIP.

Remember LoadBalancer service isn't builtin and needs to be provided by an external service like cloud.
Like Amazon has ELBs and ALBs. And you'll need to add that plugin in your kubernetes for it to work with those external LBs.
Essentially, the kube API talks to their API to control that LB.
Docker Desktop also comes with a builtin LoadBalancer service. So, you can do:
`$ kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer`

As load balancers are 3rd party, they all have varying features. In this case, the Docker Desktop LB service plugin will
publish it on the port 8888. It's the one way to have the Kubernetes on a specific port outside the default range.
It allows you to run it on port 80 or whatever. So you can do:
`$ curl localhost:8888` -> returns the response

The LoadBalancer internally auto creates NodePort & ClusterIP services.
	The LoadBalancer is accepting the packet and passing it into the NodePort which is then passing it to the ClusterIP.

That's why, `$ kubectl get services` also shows the LB service with ports something like: 8888:31933/TCP,
	cause there's always going to be that NodePort even though that's not really the port the LB is using on the localhost.

Let's clean up. You can delete multiple objects in the same command. They don't even have to be related objects.
`kubectl delete service/httpenv service/httpenv-np service/httpenv-lb deployment/httpenv`

Resources:
NodePort docs: https://kubernetes.io/docs/concepts/services-networking/service/#nodeport

124 > Kubernetes services DNS

Sometimes, we take DNS for granted cause inside docker/swarm/k8s, the DNS kinda just works.
	Cause if you curl/ping a service name, you get back the response.

Although DNS is optional/add-on, everyone puts one in there.
Starting with 1.11 release, CoreDNS is the default. Earlier KubeDNS is now deprecated.
Like Swarm, this is DNS based service discovery. When you create a service, you get the hostname that matches the service.
But here, that host name is part of the larger name the FQDN.
But if you start using namespace, you can have things with same names in different namespaces inside the same cluster.
The service FQDN:
<hostname>.<namespace>.svc.cluster.local
	- default namespace is "default"
	- the "cluster.local" is the default service DNS name given to the cluster when it was created
		if you use kubeadm to build the cluster, you can change that name, maybe if you want to create multiple clusters,
			or if you have some external requirements about how your apps bind each other.

In short, this DNS is inside the cluster only.

Resources:
Kubernetes DNS specification: https://github.com/kubernetes/dns/blob/master/docs/specification.md
CoreDNS for kubernetes: https://coredns.io/plugins/kubernetes/



[18] Kubernetes Management Techniques

125 > Section intro

Kubernetes is unopinionated and has multiple ways to use. But what's the best one? Discussed in the section.

Resources:
res/slides-kubernetes-management-techniques.pdf
res/commands-and-links-kubernetes-management-techniques.pdf

126 > YAML generator in kubectl commands

Some of the commands we've been using so far have a bit of automation behind them known as generators.
	They are the templates that create the spec to apply to the cluster based on the command line options.
	Automation, like giving the fields their defaults that you don't specify. To make the CLI easier.
		Swarm kinda does this too.

These generators can change/evolve over time. You can see what all stuff is happening behind the scene with dry-run:
`$ kubectl create deployment sample --image nginx --dry-run=client -o yaml`
`$ kubectl create job test --image nginx --dry-run=client -o yaml`
`$ kubectl expose deployment/test --port 80 --dry-run=client -o yaml` - the deployment needs to actually exist

Generators are kinda slightly opinionated defaults but still work for most use cases.

Resources:
kubectl usage conventions: https://kubernetes.io/docs/reference/kubectl/conventions/

127 > Imperative vs declarative

Meaning in terms of software:
Imperative: Focus is on how the program operates. Each step in order one after the other.
Example, to get a coffee: you need to think of every step of how you will create the coffee.

Declarative: More about what the program should accomplish. What are it's end goals.
	We don't care so much how it gets there, just that it does get there.
Example, to get a coffee: you describe the coffee to the barista who then figures how to produce it.


In Kubernetes:

Imperative example: `run`, `create deployment`, `update` commands
	- We start we the state we know(like no deployment exists).
	- We order it to create a new deployment
Different commands are required to change the deployment.
Different commands are required per object.
Easier when:
	- when you know the state
	- getting started
	- for humans at the CLI
But, it's not easy to automate.

Declarative:
We don't know the state. Do they have to make the coffee from scratch or have it already made? We just know the end result.

Example: `kubectl apply -f my-resources.yml`
	- we don't know the current state
	- we only know what we want and the end result to be(YAML contents)
Same command each time(tiny exceptions for delete).
Single or multiple YAML files are supported. Can apply entire directory.
Easiest way to automate.
Also the easiest way to GitOps happiness.
	TODO: Google GitOps. It's a type of doing dev ops using git as the source of truth.

128 > Three management approaches

3 approaches.
1. Imperative commands - what we've been doing so far.
2. Imperative objects: you can use the create commands with the yaml files:
	- `create -f file.yml`:
		It will create the objects in the files, you can have anything:
			deployment + service, storage needs, networking setups etc.
	- `replace -f file.yml`:
		You can then edit the file to replace the objects.
	- `delete ...`: and delete them

	Good for prod of small environments.
	It's kind of middle ground. You are still using YAML => you have documented it and you have the change record in git.
3. Declarative objects:
	- `apply -f file.yml` or dir\ or diff.
	- Best for prod, easier to automate.

Recommended to read the page on the management approaches mentioned in the resources below.

Important rule: don't mix the three approaches.
Bret's recommendations:
	- Learn the imperative CLI.
	- Stick to Declarative objects in prod.
	- Store the YAMLs in git and commit each change before you apply.
	- This trains you for later doing GitOps(where git commits are auto applied to clusters).

Resources:
Kubernetes management techniques (docs): https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/
Imperative command examples: https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/
Imperative config files (doc tutorials): https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/
Declarative config files: https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/

[19] Moving to Declarative Kubernetes YAML

129 > Section intro

TODO: ----- CONTINUE HERE -----

Resources:
res/s16-slides-moving-to-declarative-yaml.pdf
res/dm-s16-moving-to-declarative-kubernetes-yaml.txt
130 > Kubectl apply
Resources:
Declarative management of kubernetes (docs): https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/
131 > Kubectl configuration YAML
Resources:
Understanding kubernetes objects & specs (docs): https://kubernetes.io/docs/concepts/overview/working-with-objects/
132 > Building your YAML files
133 > Building your YAML specs
Resources:
Kubernetes API reference (docs): https://kubernetes.io/docs/reference/#api-reference
134 > Dry run CLI changes
135 > Dry runs and diffs
Resources:
APIServer dry run & kubectl diff: https://kubernetes.io/blog/2019/01/14/apiserver-dry-run-and-kubectl-diff/
136 > Labels and label selectors
Resources:
Label selectors (docs): https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors
Difference between labels & annotations: https://vsupalov.com/kubernetes-labels-annotations-difference/
Recommended labels: https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/
Assigning pods to nodes (docs): https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
Taints and tolarations (docs): https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/



[20] Your next steps and the future of kubernetes

137 > Section intro
Resources:
res/s17-slides-your-next-steps-and-the-future-of-kubernetes.pdf
res/dm-s17-your-next-steps-and-the-future-of-kubernetes.txt
138 > Storage in kubernetes
Resources:
Volumes (docs): https://kubernetes.io/docs/concepts/storage/volumes/
StatefulSets (docs): https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
PersistentVolume (docs): https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/
139 > Ingress
Resources:
Ingress (docs): https://kubernetes.io/docs/concepts/services-networking/ingress/
Ingress controllers (docs): https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/
Setup ingress on Minikube with nginx ingress controller (docs):
	https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/
Traefik ingress: https://doc.traefik.io/traefik/v2.0/providers/kubernetes-crd/#traefik-ingressroute-definition
140 > CRT's and the operator pattern
Resources:
Custom resources (docs): https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/
The operator pattern (docs): https://kubernetes.io/docs/concepts/extend-kubernetes/operator/
OperatorHub: https://operatorhub.io/
Awesome operators lisg: https://github.com/operator-framework/awesome-operators
141 > Higher development abstractions
Resources:
Spreadsheet of k8s deployment tools:
Deploying compose on kubernetes:
	https://docs.google.com/spreadsheets/d/1FCgqz1Ci7_VCz_wdh8vBitZ3giBtac_H8SBw4uxnrsE/edit#gid=0
Introducing kustomize: https://github.com/docker/compose-on-kubernetes/tree/master/docs
Kustomize GitHub: https://github.com/kubernetes-sigs/kustomize
Docker App GitHub: https://github.com/docker/app
CNAB spec: https://cnab.io/
142 > Kubernetes dashboard
Resources:
The kubernetes dashboard: https://github.com/kubernetes/dashboard
143 > Namespaces and context
Resources:
Configure access to multiple clusters:
	https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/
Namespace (docs): https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
144 > Future of kubernetes
Resources:
Kubernetes release notes: https://kubernetes.io/releases/notes/



[21] Automated CI Workflows
[22] GitHub Actions	Workflow Examples
[23] Docker Security Good Defaults and Tools
[24] Docker 19.03 Release New Features
[25] DovOps and Docker Clips
[26] Dockerfiles and Docker Images in 2022
[27] Dockerfiles and Docker Images in 2022





Time completed:
	46 mins skipped for later [12]
	Total time completed after [18] without [12]: 11:48
	Total time completed after [18] including [12]: 12:32



Cool stuff:
`$ imgcat` displays images in terminal!



